{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKQ0OESx9dy4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FORWARD PROP ON FIRST TRAINING EXAMPLE TO MATCH THE RESULTS**"
      ],
      "metadata": {
        "id": "P7dENQPH_gW6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7irlWXS_fVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNR4Yt6is5Nh",
        "outputId": "50648e0c-62a0-4217-ec24-eee28d14fb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing mnist_mlp.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile mnist_mlp.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <npy.hpp> // Include npy.hpp for loading .npy files\n",
        "\n",
        "#define INPUT_SIZE 784\n",
        "#define HIDDEN1_SIZE 512\n",
        "#define HIDDEN2_SIZE 256\n",
        "#define OUTPUT_SIZE 10\n",
        "\n",
        "__device__ float relu(float x) {\n",
        "    return fmaxf(0.0f, x);\n",
        "}\n",
        "\n",
        "__device__ float softmax(float* output, int idx, int size) {\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        sum += expf(output[i]);\n",
        "    }\n",
        "    return expf(output[idx]) / sum;\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation_relu(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[i];\n",
        "        }\n",
        "        output[idx] = relu(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[i];\n",
        "        }\n",
        "        output[idx] = z;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void softmax_layer(float *input, float *output, int size) {\n",
        "    float max_val = input[0];\n",
        "    for (int i = 1; i < size; ++i) {\n",
        "        if (input[i] > max_val) max_val = input[i];\n",
        "    }\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output[i] = expf(input[i] - max_val);\n",
        "        sum += output[i];\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output[i] /= sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void print_probabilities(float *output, int size) {\n",
        "    printf(\"Class probabilities:\\n\");\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        printf(\"Class %d: %f\\n\", i, output[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void get_predicted_class(float *output, int *predicted_class, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx == 0) {\n",
        "        int class_idx = 0;\n",
        "        float max_val = output[0];\n",
        "        for (int i = 1; i < output_size; ++i) {\n",
        "            if (output[i] > max_val) {\n",
        "                max_val = output[i];\n",
        "                class_idx = i;\n",
        "            }\n",
        "        }\n",
        "        *predicted_class = class_idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void compute_cross_entropy_loss(float *output, double *labels, float *loss, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx == 0) {\n",
        "        // Find the true class index from the one-hot encoded label\n",
        "        int true_class_idx = -1;\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            if (labels[i] == 1.0) {\n",
        "                true_class_idx = i;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (true_class_idx != -1) {\n",
        "            // Compute cross-entropy loss\n",
        "            float prob = output[true_class_idx];\n",
        "            if (prob > 0) {\n",
        "                *loss = -logf(prob);\n",
        "            } else {\n",
        "                *loss = 0.0f;  // Handle case where probability is zero\n",
        "            }\n",
        "        } else {\n",
        "            *loss = 0.0f;  // Handle the case where no valid class is found\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int input_size = INPUT_SIZE;\n",
        "    const int hidden1_size = HIDDEN1_SIZE;\n",
        "    const int hidden2_size = HIDDEN2_SIZE;\n",
        "    const int output_size = OUTPUT_SIZE;\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    std::vector<float> host_input;\n",
        "    std::vector<float> host_hidden1_weights;\n",
        "    std::vector<float> host_hidden1_biases;\n",
        "    std::vector<float> host_hidden2_weights;\n",
        "    std::vector<float> host_hidden2_biases;\n",
        "    std::vector<float> host_output_weights;\n",
        "    std::vector<float> host_output_biases;\n",
        "    std::vector<double> host_labels;  // One-hot encoded labels\n",
        "\n",
        "    std::vector<unsigned long> shape;\n",
        "\n",
        "    // Load MNIST data\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\", shape, host_input);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\", shape, host_labels);\n",
        "\n",
        "    // Load weights and biases\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", shape, host_hidden1_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", shape, host_hidden1_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", shape, host_hidden2_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", shape, host_hidden2_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", shape, host_output_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", shape, host_output_biases);\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_input, *d_hidden1_output, *d_hidden2_output, *d_output_output;\n",
        "    float *d_hidden1_weights, *d_hidden1_biases, *d_hidden2_weights, *d_hidden2_biases, *d_output_weights, *d_output_biases;\n",
        "    double *d_labels;\n",
        "    float *d_loss;\n",
        "    int *d_predicted_class;\n",
        "\n",
        "    cudaMalloc((void**)&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden1_weights, input_size * hidden1_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden1_biases, hidden1_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden1_output, hidden1_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&d_hidden2_weights, hidden1_size * hidden2_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden2_biases, hidden2_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden2_output, hidden2_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&d_output_weights, hidden2_size * output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_output, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&d_labels, output_size * sizeof(double));  // One-hot encoded labels\n",
        "    cudaMalloc((void**)&d_predicted_class, sizeof(int));\n",
        "    cudaMalloc((void**)&d_loss, sizeof(float));\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_input, host_input.data(), input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_weights, host_hidden1_weights.data(), input_size * hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_biases, host_hidden1_biases.data(), hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_weights, host_hidden2_weights.data(), hidden1_size * hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_biases, host_hidden2_biases.data(), hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_weights, host_output_weights.data(), hidden2_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases.data(), output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_labels, host_labels.data(), output_size * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernels\n",
        "    int block_size = 256;\n",
        "    int grid_size = (hidden1_size + block_size - 1) / block_size;\n",
        "    linear_layer_and_activation_relu<<<grid_size, block_size>>>(d_input, d_hidden1_weights, d_hidden1_biases, d_hidden1_output, input_size, hidden1_size);\n",
        "\n",
        "    grid_size = (hidden2_size + block_size - 1) / block_size;\n",
        "    linear_layer_and_activation_relu<<<grid_size, block_size>>>(d_hidden1_output, d_hidden2_weights, d_hidden2_biases, d_hidden2_output, hidden1_size, hidden2_size);\n",
        "\n",
        "    grid_size = (output_size + block_size - 1) / block_size;\n",
        "    linear_layer_and_activation<<<grid_size, block_size>>>(d_hidden2_output, d_output_weights, d_output_biases, d_output_output, hidden2_size, output_size);\n",
        "\n",
        "    softmax_layer<<<1, 1>>>(d_output_output, d_output_output, output_size);\n",
        "    print_probabilities<<<1, 1>>>(d_output_output, output_size);\n",
        "\n",
        "    int predicted_class;\n",
        "    cudaMemcpy(d_predicted_class, &predicted_class, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    get_predicted_class<<<1, 1>>>(d_output_output, d_predicted_class, output_size);\n",
        "\n",
        "    float loss;\n",
        "    compute_cross_entropy_loss<<<1, 1>>>(d_output_output, d_labels, d_loss, output_size);\n",
        "\n",
        "    cudaMemcpy(&predicted_class, d_predicted_class, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(&loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Predicted class: \" << predicted_class << std::endl;\n",
        "    std::cout << \"Cross-entropy loss: \" << loss << std::endl;\n",
        "\n",
        "    // Copy d_labels back to host to print them\n",
        "    std::vector<double> host_labels_output(output_size);\n",
        "    cudaMemcpy(host_labels_output.data(), d_labels, output_size * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    std::cout << \"Labels (one-hot encoded):\" << std::endl;\n",
        "    for (int i = 0; i < output_size; ++i) {\n",
        "        std::cout << \"Label \" << i << \": \" << host_labels_output[i] << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden1_weights);\n",
        "    cudaFree(d_hidden1_biases);\n",
        "    cudaFree(d_hidden1_output);\n",
        "    cudaFree(d_hidden2_weights);\n",
        "    cudaFree(d_hidden2_biases);\n",
        "    cudaFree(d_hidden2_output);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_output_output);\n",
        "    cudaFree(d_labels);\n",
        "    cudaFree(d_loss);\n",
        "    cudaFree(d_predicted_class);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1l8VM28Is5Ni",
        "outputId": "29ea5257-6be5-4c9d-9015-84556a62c095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class probabilities:\n",
            "Class 0: 0.109313\n",
            "Class 1: 0.058303\n",
            "Class 2: 0.093253\n",
            "Class 3: 0.070686\n",
            "Class 4: 0.180806\n",
            "Class 5: 0.126452\n",
            "Class 6: 0.080382\n",
            "Class 7: 0.094052\n",
            "Class 8: 0.121958\n",
            "Class 9: 0.064795\n",
            "Predicted class: 4\n",
            "Cross-entropy loss: 2.06789\n",
            "Labels (one-hot encoded):\n",
            "Label 0: 0\n",
            "Label 1: 0\n",
            "Label 2: 0\n",
            "Label 3: 0\n",
            "Label 4: 0\n",
            "Label 5: 1\n",
            "Label 6: 0\n",
            "Label 7: 0\n",
            "Label 8: 0\n",
            "Label 9: 0\n"
          ]
        }
      ],
      "source": [
        "!nvcc -I/content/drive/MyDrive/Untitled_folder -o mnist_mlp mnist_mlp.cu\n",
        "\n",
        "\n",
        "!./mnist_mlp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vy1aRVEZ_yfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FORWARD PROP ON FIRST FOUR TRAINING  EXAMPLES IN BATCH**"
      ],
      "metadata": {
        "id": "iy3AuaZS_08C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COrm1ryk6fFK",
        "outputId": "33083045-c37f-4db9-9b4f-5d1a10ad06e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting mnist_mlp_2.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile mnist_mlp_2.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <npy.hpp> // Include npy.hpp for loading .npy files\n",
        "\n",
        "#define INPUT_SIZE 784\n",
        "#define HIDDEN1_SIZE 512\n",
        "#define HIDDEN2_SIZE 256\n",
        "#define OUTPUT_SIZE 10\n",
        "#define MINI_BATCH_SIZE 5\n",
        "\n",
        "__device__ float relu(float x) {\n",
        "    return fmaxf(0.0f, x);\n",
        "}\n",
        "\n",
        "__device__ float softmax(float* output, int idx, int size) {\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        sum += expf(output[i]);\n",
        "    }\n",
        "    return expf(output[idx]) / sum;\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation_relu(float *input, float *weights, float *biases, float *output, int input_size, int output_size, int batch_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int batch_idx = blockIdx.y;\n",
        "    if (idx < output_size && batch_idx < batch_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[batch_idx * input_size + i];\n",
        "        }\n",
        "        output[batch_idx * output_size + idx] = relu(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size, int batch_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int batch_idx = blockIdx.y;\n",
        "    if (idx < output_size && batch_idx < batch_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[batch_idx * input_size + i];\n",
        "        }\n",
        "        output[batch_idx * output_size + idx] = z;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void softmax_layer(float *input, float *output, int size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    if (batch_idx < batch_size) {\n",
        "        float max_val = input[batch_idx * size];\n",
        "        for (int i = 1; i < size; ++i) {\n",
        "            if (input[batch_idx * size + i] > max_val) max_val = input[batch_idx * size + i];\n",
        "        }\n",
        "\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            output[batch_idx * size + i] = expf(input[batch_idx * size + i] - max_val);\n",
        "            sum += output[batch_idx * size + i];\n",
        "        }\n",
        "\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            output[batch_idx * size + i] /= sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void print_probabilities(float *output, int size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    if (batch_idx < batch_size) {\n",
        "        printf(\"Class probabilities for image %d:\\n\", batch_idx);\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            printf(\"Class %d: %f\\n\", i, output[batch_idx * size + i]);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void get_predicted_class(float *output, int *predicted_class, int output_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    if (batch_idx < batch_size) {\n",
        "        int class_idx = 0;\n",
        "        float max_val = output[batch_idx * output_size];\n",
        "        for (int i = 1; i < output_size; ++i) {\n",
        "            if (output[batch_idx * output_size + i] > max_val) {\n",
        "                max_val = output[batch_idx * output_size + i];\n",
        "                class_idx = i;\n",
        "            }\n",
        "        }\n",
        "        predicted_class[batch_idx] = class_idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void compute_cross_entropy_loss(float *output, double *labels, float *loss, int size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    if (batch_idx < batch_size) {\n",
        "        // Find the true class index from the one-hot encoded label\n",
        "        int true_class_idx = -1;\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            if (labels[batch_idx * size + i] == 1.0) {\n",
        "                true_class_idx = i;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if (true_class_idx != -1) {\n",
        "            // Compute cross-entropy loss\n",
        "            float prob = output[batch_idx * size + true_class_idx];\n",
        "            loss[batch_idx] = (prob > 0) ? -logf(prob) : 0.0f;\n",
        "        } else {\n",
        "            loss[batch_idx] = 0.0f;  // Handle the case where no valid class is found\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int input_size = INPUT_SIZE;\n",
        "    const int hidden1_size = HIDDEN1_SIZE;\n",
        "    const int hidden2_size = HIDDEN2_SIZE;\n",
        "    const int output_size = OUTPUT_SIZE;\n",
        "    const int batch_size = MINI_BATCH_SIZE;\n",
        "\n",
        "    // Allocate memory on the host\n",
        "    std::vector<float> host_input;\n",
        "    std::vector<float> host_hidden1_weights;\n",
        "    std::vector<float> host_hidden1_biases;\n",
        "    std::vector<float> host_hidden2_weights;\n",
        "    std::vector<float> host_hidden2_biases;\n",
        "    std::vector<float> host_output_weights;\n",
        "    std::vector<float> host_output_biases;\n",
        "    std::vector<double> host_labels;  // One-hot encoded labels\n",
        "\n",
        "    std::vector<unsigned long> shape;\n",
        "\n",
        "    // Load MNIST data\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\", shape, host_input);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\", shape, host_labels);\n",
        "\n",
        "    // Load weights and biases\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", shape, host_hidden1_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", shape, host_hidden1_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", shape, host_hidden2_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", shape, host_hidden2_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", shape, host_output_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", shape, host_output_biases);\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_input, *d_hidden1_output, *d_hidden2_output, *d_output_output;\n",
        "    float *d_hidden1_weights, *d_hidden1_biases, *d_hidden2_weights, *d_hidden2_biases, *d_output_weights, *d_output_biases;\n",
        "    double *d_labels;\n",
        "    float *d_loss;\n",
        "    int *d_predicted_class;\n",
        "\n",
        "    cudaMalloc((void**)&d_input, input_size * batch_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden1_weights, input_size * hidden1_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden1_biases, hidden1_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden1_output, hidden1_size * batch_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&d_hidden2_weights, hidden1_size * hidden2_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden2_biases, hidden2_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden2_output, hidden2_size * batch_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&d_output_weights, hidden2_size * output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_output, output_size * batch_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc((void**)&d_labels, output_size * batch_size * sizeof(double));  // One-hot encoded labels\n",
        "    cudaMalloc((void**)&d_predicted_class, batch_size * sizeof(int));\n",
        "    cudaMalloc((void**)&d_loss, batch_size * sizeof(float));\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_input, host_input.data(), input_size * batch_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_weights, host_hidden1_weights.data(), input_size * hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_biases, host_hidden1_biases.data(), hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_weights, host_hidden2_weights.data(), hidden1_size * hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_biases, host_hidden2_biases.data(), hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_weights, host_output_weights.data(), hidden2_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases.data(), output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_labels, host_labels.data(), output_size * batch_size * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Compute hidden1 layer\n",
        "    dim3 threadsPerBlock(32);\n",
        "    dim3 numBlocks((hidden1_size + threadsPerBlock.x - 1) / threadsPerBlock.x, batch_size);\n",
        "    linear_layer_and_activation_relu<<<numBlocks, threadsPerBlock>>>(d_input, d_hidden1_weights, d_hidden1_biases, d_hidden1_output, input_size, hidden1_size, batch_size);\n",
        "\n",
        "    // Compute hidden2 layer\n",
        "    numBlocks = dim3((hidden2_size + threadsPerBlock.x - 1) / threadsPerBlock.x, batch_size);\n",
        "    linear_layer_and_activation_relu<<<numBlocks, threadsPerBlock>>>(d_hidden1_output, d_hidden2_weights, d_hidden2_biases, d_hidden2_output, hidden1_size, hidden2_size, batch_size);\n",
        "\n",
        "    // Compute output layer\n",
        "    numBlocks = dim3((output_size + threadsPerBlock.x - 1) / threadsPerBlock.x, batch_size);\n",
        "    linear_layer_and_activation<<<numBlocks, threadsPerBlock>>>(d_hidden2_output, d_output_weights, d_output_biases, d_output_output, hidden2_size, output_size, batch_size);\n",
        "\n",
        "    // Apply softmax\n",
        "    numBlocks = batch_size;\n",
        "    softmax_layer<<<numBlocks, 1>>>(d_output_output, d_output_output, output_size, batch_size);\n",
        "\n",
        "    // Compute cross-entropy loss\n",
        "    compute_cross_entropy_loss<<<numBlocks, 1>>>(d_output_output, d_labels, d_loss, output_size, batch_size);\n",
        "\n",
        "    // Get predicted class\n",
        "    get_predicted_class<<<numBlocks, 1>>>(d_output_output, d_predicted_class, output_size, batch_size);\n",
        "\n",
        "    // Copy results back to host\n",
        "    std::vector<float> host_loss(batch_size);\n",
        "    std::vector<int> host_predicted_class(batch_size);\n",
        "    cudaMemcpy(host_loss.data(), d_loss, batch_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    cudaMemcpy(host_predicted_class.data(), d_predicted_class, batch_size * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print predictions and loss\n",
        "    for (int i = 0; i < batch_size; ++i) {\n",
        "        std::cout << \"Image \" << i << \":\\n\";\n",
        "        std::cout << \"Loss: \" << host_loss[i] << \"\\n\";\n",
        "        std::cout << \"Predicted class: \" << host_predicted_class[i] << \"\\n\";\n",
        "\n",
        "        // Print actual class from one-hot encoded labels\n",
        "        int actual_class = -1;\n",
        "        for (int j = 0; j < output_size; ++j) {\n",
        "            if (host_labels[i * output_size + j] == 1.0) {\n",
        "                actual_class = j;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        std::cout << \"Actual class: \" << actual_class << \"\\n\";\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden1_weights);\n",
        "    cudaFree(d_hidden1_biases);\n",
        "    cudaFree(d_hidden1_output);\n",
        "    cudaFree(d_hidden2_weights);\n",
        "    cudaFree(d_hidden2_biases);\n",
        "    cudaFree(d_hidden2_output);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_output_output);\n",
        "    cudaFree(d_labels);\n",
        "    cudaFree(d_predicted_class);\n",
        "    cudaFree(d_loss);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpO9giIZ6f1q",
        "outputId": "24730657-4769-42bd-ffa8-57945bd2f58e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image 0:\n",
            "Loss: 2.06789\n",
            "Predicted class: 4\n",
            "Actual class: 5\n",
            "Image 1:\n",
            "Loss: 2.61446\n",
            "Predicted class: 4\n",
            "Actual class: 0\n",
            "Image 2:\n",
            "Loss: 2.03958\n",
            "Predicted class: 4\n",
            "Actual class: 4\n",
            "Image 3:\n",
            "Loss: 2.29785\n",
            "Predicted class: 5\n",
            "Actual class: 1\n",
            "Image 4:\n",
            "Loss: 2.65076\n",
            "Predicted class: 4\n",
            "Actual class: 9\n"
          ]
        }
      ],
      "source": [
        "!nvcc -I/content/drive/MyDrive/Untitled_folder -o mnist_mlp_2 mnist_mlp_2.cu\n",
        "!./mnist_mlp_2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BACKWARD PROPAGATION ON SINGLE TRAINING EXAMPLE**"
      ],
      "metadata": {
        "id": "fFEmGyWiAMPD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPNacoTL_p5n",
        "outputId": "658501d1-53ad-445a-e36d-23c13341911a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting check.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile check.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <npy.hpp> // Include npy.hpp for loading .npy files\n",
        "\n",
        "#define INPUT_SIZE 784\n",
        "#define HIDDEN1_SIZE 512\n",
        "#define HIDDEN2_SIZE 256\n",
        "#define OUTPUT_SIZE 10\n",
        "\n",
        "__device__ float relu(float x) {\n",
        "    return fmaxf(0.0f, x);\n",
        "}\n",
        "\n",
        "__device__ float relu_derivative(float x) {\n",
        "    return x > 0.0f ? 1.0f : 0.0f;\n",
        "}\n",
        "\n",
        "__device__ float softmax(float* output, int idx, int size) {\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        sum += expf(output[i]);\n",
        "    }\n",
        "    return expf(output[idx]) / sum;\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation_relu(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[i];\n",
        "        }\n",
        "        output[idx] = relu(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[i];\n",
        "        }\n",
        "        output[idx] = z;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void softmax_layer(float *input, float *output, int size) {\n",
        "    float max_val = input[0];\n",
        "    for (int i = 1; i < size; ++i) {\n",
        "        if (input[i] > max_val) max_val = input[i];\n",
        "    }\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output[i] = expf(input[i] - max_val);\n",
        "        sum += output[i];\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output[i] /= sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void print_probabilities(float *output, int size) {\n",
        "    printf(\"Class probabilities:\\n\");\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        printf(\"Class %d: %f\\n\", i, output[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void get_predicted_class(float *output, int *predicted_class, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx == 0) {\n",
        "        int class_idx = 0;\n",
        "        float max_val = output[0];\n",
        "        for (int i = 1; i < output_size; ++i) {\n",
        "            if (output[i] > max_val) {\n",
        "                max_val = output[i];\n",
        "                class_idx = i;\n",
        "            }\n",
        "        }\n",
        "        *predicted_class = class_idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void compute_cross_entropy_loss(float *output, double *labels, float *loss, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx == 0) {\n",
        "        int true_class_idx = -1;\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            if (labels[i] == 1.0) {\n",
        "                true_class_idx = i;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        if (true_class_idx != -1) {\n",
        "            float prob = output[true_class_idx];\n",
        "            if (prob > 0) {\n",
        "                *loss = -logf(prob);\n",
        "            } else {\n",
        "                *loss = 0.0f;\n",
        "            }\n",
        "        } else {\n",
        "            *loss = 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_output(float *d_output, double *d_labels, float *d_dz3, float *d_dW3, float *d_db3, float *d_a2, int hidden2_size, int output_size) {\n",
        "    int output_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (output_idx < output_size) {\n",
        "        // Compute error in the output layer (dz3)\n",
        "        d_dz3[output_idx] = d_output[output_idx] - d_labels[output_idx];\n",
        "\n",
        "        // Compute bias gradient (db3)\n",
        "        d_db3[output_idx] = d_dz3[output_idx];\n",
        "\n",
        "        // Compute weight gradients (dW3), iterate over the hidden2_size for the outer product\n",
        "        for (int hidden_idx = 0; hidden_idx < hidden2_size; ++hidden_idx) {\n",
        "            d_dW3[hidden_idx * output_size + output_idx] = d_a2[hidden_idx] * d_dz3[output_idx];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_hidden2(float *d_dz3, float *d_W3, float *d_a2, float *d_dz2, float *d_dW2, float *d_db2, int hidden1_size, int hidden2_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden2_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < OUTPUT_SIZE; ++i) {\n",
        "            sum += d_dz3[i] * d_W3[idx * OUTPUT_SIZE + i];\n",
        "        }\n",
        "        d_dz2[idx] = relu_derivative(d_a2[idx]) * sum;\n",
        "\n",
        "        // Accumulate gradients for weights\n",
        "        d_dW2[idx] = 0.0f;\n",
        "        for (int i = 0; i < hidden1_size; ++i) {\n",
        "            d_dW2[idx * hidden1_size + i] = d_dz2[idx] * d_a2[i];\n",
        "        }\n",
        "\n",
        "        // Accumulate gradients for biases\n",
        "        d_db2[idx] = d_dz2[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__ void backpropagation_hidden1(float *d_dz2, float *d_W2, float *d_x, float *d_dz1, float *d_dW1, float *d_db1, int input_size, int hidden1_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden1_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < HIDDEN2_SIZE; ++i) {\n",
        "            sum += d_dz2[i] * d_W2[idx * HIDDEN2_SIZE + i];\n",
        "        }\n",
        "        d_dz1[idx] = relu_derivative(d_x[idx]) * sum;\n",
        "        d_dW1[idx] = d_dz1[idx];\n",
        "        d_db1[idx] = d_dz1[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void update_weights_biases(float *d_W1, float *d_b1, float *d_W2, float *d_b2, float *d_W3, float *d_b3,\n",
        "                                      float *d_dW1, float *d_db1, float *d_dW2, float *d_db2, float *d_dW3, float *d_db3,\n",
        "                                      float learning_rate, int input_size, int hidden1_size, int hidden2_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < input_size * hidden1_size) {\n",
        "        d_W1[idx] -= learning_rate * d_dW1[idx];\n",
        "    }\n",
        "    if (idx < hidden1_size) {\n",
        "        d_b1[idx] -= learning_rate * d_db1[idx];\n",
        "    }\n",
        "    if (idx < hidden1_size * hidden2_size) {\n",
        "        d_W2[idx] -= learning_rate * d_dW2[idx];\n",
        "    }\n",
        "    if (idx < hidden2_size) {\n",
        "        d_b2[idx] -= learning_rate * d_db2[idx];\n",
        "    }\n",
        "    if (idx < hidden2_size * output_size) {\n",
        "        d_W3[idx] -= learning_rate * d_dW3[idx];\n",
        "    }\n",
        "    if (idx < output_size) {\n",
        "        d_b3[idx] -= learning_rate * d_db3[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int input_size = INPUT_SIZE;\n",
        "    const int hidden1_size = HIDDEN1_SIZE;\n",
        "    const int hidden2_size = HIDDEN2_SIZE;\n",
        "    const int output_size = OUTPUT_SIZE;\n",
        "    const float learning_rate = 0.01f;\n",
        "\n",
        "    std::vector<float> host_input;\n",
        "    std::vector<float> host_hidden1_weights;\n",
        "    std::vector<float> host_hidden1_biases;\n",
        "    std::vector<float> host_hidden2_weights;\n",
        "    std::vector<float> host_hidden2_biases;\n",
        "    std::vector<float> host_output_weights;\n",
        "    std::vector<float> host_output_biases;\n",
        "    std::vector<double> host_labels;\n",
        "\n",
        "    std::vector<unsigned long> shape;\n",
        "\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\", shape, host_input);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\", shape, host_labels);\n",
        "\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", shape, host_hidden1_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", shape, host_hidden1_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", shape, host_hidden2_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", shape, host_hidden2_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", shape, host_output_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", shape, host_output_biases);\n",
        "\n",
        "    float *d_input, *d_hidden1_weights, *d_hidden1_biases;\n",
        "    float *d_hidden2_weights, *d_hidden2_biases, *d_output_weights, *d_output_biases;\n",
        "    float *d_hidden1_output, *d_hidden2_output, *d_output, *d_softmax_output;\n",
        "    float *d_dz1, *d_dz2, *d_dz3;\n",
        "    float *d_dW1, *d_dW2, *d_dW3;\n",
        "    float *d_db1, *d_db2, *d_db3;\n",
        "    int *d_predicted_class;\n",
        "    float *d_loss;\n",
        "    double *d_labels;\n",
        "\n",
        "    cudaMalloc(&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden1_weights, input_size * hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden1_biases, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden2_weights, hidden1_size * hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden2_biases, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_output_weights, hidden2_size * output_size * sizeof(float));\n",
        "    cudaMalloc(&d_output_biases, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_hidden1_output, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden2_output, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_output, output_size * sizeof(float));\n",
        "    cudaMalloc(&d_softmax_output, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_dz1, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_dz2, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_dz3, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_dW1, input_size * hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_dW2, hidden1_size * hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_dW3, hidden2_size * output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_db1, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_db2, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_db3, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_predicted_class, sizeof(int));\n",
        "    cudaMalloc(&d_loss, sizeof(float));\n",
        "    cudaMalloc(&d_labels, output_size * sizeof(double));\n",
        "\n",
        "    cudaMemcpy(d_input, host_input.data(), input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_weights, host_hidden1_weights.data(), input_size * hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_biases, host_hidden1_biases.data(), hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_weights, host_hidden2_weights.data(), hidden1_size * hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_biases, host_hidden2_biases.data(), hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_weights, host_output_weights.data(), hidden2_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases.data(), output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaMemcpy(d_labels, host_labels.data(), output_size * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int epochs = 35;\n",
        "\n",
        "    for (int epoch = 0; epoch < epochs; ++epoch) {\n",
        "        // Forward pass\n",
        "        linear_layer_and_activation_relu<<<(hidden1_size + 255) / 256, 256>>>(d_input, d_hidden1_weights, d_hidden1_biases, d_hidden1_output, input_size, hidden1_size);\n",
        "        linear_layer_and_activation_relu<<<(hidden2_size + 255) / 256, 256>>>(d_hidden1_output, d_hidden2_weights, d_hidden2_biases, d_hidden2_output, hidden1_size, hidden2_size);\n",
        "        linear_layer_and_activation<<<(output_size + 255) / 256, 256>>>(d_hidden2_output, d_output_weights, d_output_biases, d_output, hidden2_size, output_size);\n",
        "        softmax_layer<<<(output_size + 255) / 256, 256>>>(d_output, d_softmax_output, output_size);\n",
        "\n",
        "        // Compute loss\n",
        "        compute_cross_entropy_loss<<<1, 1>>>(d_softmax_output, d_labels, d_loss, output_size);\n",
        "        float loss;\n",
        "        cudaMemcpy(&loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        std::cout << \"Epoch \" << epoch + 1 << \", Loss: \" << loss << std::endl;\n",
        "\n",
        "        // Backpropagation\n",
        "        backpropagation_output<<<(output_size + 255) / 256, 256>>>(d_softmax_output, d_labels, d_dz3, d_dW3, d_db3, d_hidden2_output, hidden2_size, output_size);\n",
        "        backpropagation_hidden2<<<(hidden2_size + 255) / 256, 256>>>(d_dz3, d_output_weights, d_hidden2_output, d_dz2, d_dW2, d_db2, hidden1_size, hidden2_size);\n",
        "        backpropagation_hidden1<<<(hidden1_size + 255) / 256, 256>>>(d_dz2, d_hidden2_weights, d_input, d_dz1, d_dW1, d_db1, input_size, hidden1_size);\n",
        "\n",
        "        // Update weights and biases\n",
        "        update_weights_biases<<<(input_size * hidden1_size + 255) / 256, 256>>>(d_hidden1_weights, d_hidden1_biases, d_hidden2_weights, d_hidden2_biases, d_output_weights, d_output_biases,\n",
        "                                                                               d_dW1, d_db1, d_dW2, d_db2, d_dW3, d_db3, learning_rate,\n",
        "                                                                               input_size, hidden1_size, hidden2_size, output_size);\n",
        "\n",
        "        // Synchronize before printing\n",
        "        cudaDeviceSynchronize();\n",
        "     }\n",
        "\n",
        "    // Free resources\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden1_weights);\n",
        "    cudaFree(d_hidden1_biases);\n",
        "    cudaFree(d_hidden2_weights);\n",
        "\n",
        "    cudaFree(d_hidden2_biases);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "\n",
        "    cudaFree(d_hidden1_output);\n",
        "    cudaFree(d_hidden2_output);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_softmax_output);\n",
        "\n",
        "    cudaFree(d_dz1);\n",
        "    cudaFree(d_dz2);\n",
        "    cudaFree(d_dz3);\n",
        "\n",
        "    cudaFree(d_dW1);\n",
        "    cudaFree(d_dW2);\n",
        "    cudaFree(d_dW3);\n",
        "\n",
        "    cudaFree(d_db1);\n",
        "    cudaFree(d_db2);\n",
        "    cudaFree(d_db3);\n",
        "\n",
        "    cudaFree(d_predicted_class);\n",
        "    cudaFree(d_loss);\n",
        "    cudaFree(d_labels);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDWfqHdFCbjY",
        "outputId": "164623c5-db99-4caa-a18c-c7ada27682a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.06789\n",
            "Epoch 2, Loss: 1.94808\n",
            "Epoch 3, Loss: 1.83185\n",
            "Epoch 4, Loss: 1.71941\n",
            "Epoch 5, Loss: 1.61098\n",
            "Epoch 6, Loss: 1.50676\n",
            "Epoch 7, Loss: 1.40699\n",
            "Epoch 8, Loss: 1.31186\n",
            "Epoch 9, Loss: 1.22156\n",
            "Epoch 10, Loss: 1.13621\n",
            "Epoch 11, Loss: 1.05589\n",
            "Epoch 12, Loss: 0.980652\n",
            "Epoch 13, Loss: 0.910453\n",
            "Epoch 14, Loss: 0.845214\n",
            "Epoch 15, Loss: 0.784864\n",
            "Epoch 16, Loss: 0.729481\n",
            "Epoch 17, Loss: 0.678495\n",
            "Epoch 18, Loss: 0.631655\n",
            "Epoch 19, Loss: 0.588696\n",
            "Epoch 20, Loss: 0.549342\n",
            "Epoch 21, Loss: 0.513315\n",
            "Epoch 22, Loss: 0.480346\n",
            "Epoch 23, Loss: 0.450176\n",
            "Epoch 24, Loss: 0.42256\n",
            "Epoch 25, Loss: 0.397267\n",
            "Epoch 26, Loss: 0.374084\n",
            "Epoch 27, Loss: 0.352816\n",
            "Epoch 28, Loss: 0.333283\n",
            "Epoch 29, Loss: 0.315322\n",
            "Epoch 30, Loss: 0.298756\n",
            "Epoch 31, Loss: 0.283507\n",
            "Epoch 32, Loss: 0.269427\n",
            "Epoch 33, Loss: 0.256405\n",
            "Epoch 34, Loss: 0.244345\n",
            "Epoch 35, Loss: 0.233158\n"
          ]
        }
      ],
      "source": [
        "!nvcc -I/content/drive/MyDrive/Untitled_folder -o check check.cu\n",
        "!./check"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BACKWARD PROPAGATION ON COMPLETE DATASET(Sample by sample)**"
      ],
      "metadata": {
        "id": "A6ahmLFuWZYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile back.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <npy.hpp> // Include npy.hpp for loading .npy files\n",
        "\n",
        "#define INPUT_SIZE 784\n",
        "#define HIDDEN1_SIZE 512\n",
        "#define HIDDEN2_SIZE 256\n",
        "#define OUTPUT_SIZE 10\n",
        "\n",
        "__device__ float relu(float x) {\n",
        "    return fmaxf(0.0f, x);\n",
        "}\n",
        "\n",
        "__device__ float relu_derivative(float x) {\n",
        "    return x > 0.0f ? 1.0f : 0.0f;\n",
        "}\n",
        "\n",
        "__device__ float softmax(float* output, int idx, int size) {\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        sum += expf(output[i]);\n",
        "    }\n",
        "    return expf(output[idx]) / sum;\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation_relu(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[i];\n",
        "        }\n",
        "        output[idx] = relu(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[i];\n",
        "        }\n",
        "        output[idx] = z;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void softmax_layer(float *input, float *output, int size) {\n",
        "    float max_val = input[0];\n",
        "    for (int i = 1; i < size; ++i) {\n",
        "        if (input[i] > max_val) max_val = input[i];\n",
        "    }\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output[i] = expf(input[i] - max_val);\n",
        "        sum += output[i];\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output[i] /= sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void print_probabilities(float *output, int size) {\n",
        "    printf(\"Class probabilities:\\n\");\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        printf(\"Class %d: %f\\n\", i, output[i]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void get_predicted_class(float *output, int *predicted_class, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx == 0) {\n",
        "        int class_idx = 0;\n",
        "        float max_val = output[0];\n",
        "        for (int i = 1; i < output_size; ++i) {\n",
        "            if (output[i] > max_val) {\n",
        "                max_val = output[i];\n",
        "                class_idx = i;\n",
        "            }\n",
        "        }\n",
        "        *predicted_class = class_idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void compute_cross_entropy_loss(float *output, double *labels, float *loss, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx == 0) {\n",
        "        int true_class_idx = -1;\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            if (labels[i] == 1.0) {\n",
        "                true_class_idx = i;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        if (true_class_idx != -1) {\n",
        "            float prob = output[true_class_idx];\n",
        "            if (prob > 0) {\n",
        "                *loss = -logf(prob);\n",
        "            } else {\n",
        "                *loss = 0.0f;\n",
        "            }\n",
        "        } else {\n",
        "            *loss = 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_output(float *d_output, double *d_labels, float *d_dz3, float *d_dW3, float *d_db3, float *d_a2, int hidden2_size, int output_size) {\n",
        "    int output_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (output_idx < output_size) {\n",
        "        // Compute error in the output layer (dz3)\n",
        "        d_dz3[output_idx] = d_output[output_idx] - d_labels[output_idx];\n",
        "\n",
        "        // Compute bias gradient (db3)\n",
        "        d_db3[output_idx] = d_dz3[output_idx];\n",
        "\n",
        "        // Compute weight gradients (dW3), iterate over the hidden2_size for the outer product\n",
        "        for (int hidden_idx = 0; hidden_idx < hidden2_size; ++hidden_idx) {\n",
        "            d_dW3[hidden_idx * output_size + output_idx] = d_a2[hidden_idx] * d_dz3[output_idx];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_hidden2(float *d_dz3, float *d_W3, float *d_a2, float *d_dz2, float *d_dW2, float *d_db2, int hidden1_size, int hidden2_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden2_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < OUTPUT_SIZE; ++i) {\n",
        "            sum += d_dz3[i] * d_W3[idx * OUTPUT_SIZE + i];\n",
        "        }\n",
        "        d_dz2[idx] = relu_derivative(d_a2[idx]) * sum;\n",
        "\n",
        "        // Accumulate gradients for weights\n",
        "        d_dW2[idx] = 0.0f;\n",
        "        for (int i = 0; i < hidden1_size; ++i) {\n",
        "            d_dW2[idx * hidden1_size + i] = d_dz2[idx] * d_a2[i];\n",
        "        }\n",
        "\n",
        "        // Accumulate gradients for biases\n",
        "        d_db2[idx] = d_dz2[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "__global__ void backpropagation_hidden1(float *d_dz2, float *d_W2, float *d_x, float *d_dz1, float *d_dW1, float *d_db1, int input_size, int hidden1_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden1_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < HIDDEN2_SIZE; ++i) {\n",
        "            sum += d_dz2[i] * d_W2[idx * HIDDEN2_SIZE + i];\n",
        "        }\n",
        "        d_dz1[idx] = relu_derivative(d_x[idx]) * sum;\n",
        "        d_dW1[idx] = d_dz1[idx];\n",
        "        d_db1[idx] = d_dz1[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void update_weights_biases(float *d_W1, float *d_b1, float *d_W2, float *d_b2, float *d_W3, float *d_b3,\n",
        "                                      float *d_dW1, float *d_db1, float *d_dW2, float *d_db2, float *d_dW3, float *d_db3,\n",
        "                                      float learning_rate, int input_size, int hidden1_size, int hidden2_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < input_size * hidden1_size) {\n",
        "        d_W1[idx] -= learning_rate * d_dW1[idx];\n",
        "    }\n",
        "    if (idx < hidden1_size) {\n",
        "        d_b1[idx] -= learning_rate * d_db1[idx];\n",
        "    }\n",
        "    if (idx < hidden1_size * hidden2_size) {\n",
        "        d_W2[idx] -= learning_rate * d_dW2[idx];\n",
        "    }\n",
        "    if (idx < hidden2_size) {\n",
        "        d_b2[idx] -= learning_rate * d_db2[idx];\n",
        "    }\n",
        "    if (idx < hidden2_size * output_size) {\n",
        "        d_W3[idx] -= learning_rate * d_dW3[idx];\n",
        "    }\n",
        "    if (idx < output_size) {\n",
        "        d_b3[idx] -= learning_rate * d_db3[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    const int input_size = INPUT_SIZE;\n",
        "    const int hidden1_size = HIDDEN1_SIZE;\n",
        "    const int hidden2_size = HIDDEN2_SIZE;\n",
        "    const int output_size = OUTPUT_SIZE;\n",
        "    const float learning_rate = 0.001f;\n",
        "\n",
        "    std::vector<float> host_input;\n",
        "    std::vector<float> host_hidden1_weights;\n",
        "    std::vector<float> host_hidden1_biases;\n",
        "    std::vector<float> host_hidden2_weights;\n",
        "    std::vector<float> host_hidden2_biases;\n",
        "    std::vector<float> host_output_weights;\n",
        "    std::vector<float> host_output_biases;\n",
        "    std::vector<double> host_labels;\n",
        "\n",
        "    std::vector<unsigned long> shape;\n",
        "\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\", shape, host_input);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\", shape, host_labels);\n",
        "\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", shape, host_hidden1_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", shape, host_hidden1_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", shape, host_hidden2_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", shape, host_hidden2_biases);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", shape, host_output_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", shape, host_output_biases);\n",
        "\n",
        "    // Allocate device memory\n",
        "    float *d_input, *d_hidden1_weights, *d_hidden1_biases, *d_hidden2_weights, *d_hidden2_biases, *d_output_weights, *d_output_biases;\n",
        "    double *d_labels;\n",
        "    float *d_hidden1_output, *d_hidden2_output, *d_output;\n",
        "    float *d_dW1, *d_db1, *d_dW2, *d_db2, *d_dW3, *d_db3;\n",
        "    float *d_dz1, *d_dz2, *d_dz3;\n",
        "    float *d_loss;\n",
        "    int *d_predicted_class;\n",
        "    int *predicted_class = new int;\n",
        "\n",
        "    cudaMalloc(&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden1_weights, input_size * hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden1_biases, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden2_weights, hidden1_size * hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden2_biases, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_output_weights, hidden2_size * output_size * sizeof(float));\n",
        "    cudaMalloc(&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc(&d_labels, output_size * sizeof(double));\n",
        "\n",
        "    cudaMalloc(&d_hidden1_output, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_hidden2_output, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_output, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_dW1, input_size * hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_db1, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_dW2, hidden1_size * hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_db2, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_dW3, hidden2_size * output_size * sizeof(float));\n",
        "    cudaMalloc(&d_db3, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_dz1, hidden1_size * sizeof(float));\n",
        "    cudaMalloc(&d_dz2, hidden2_size * sizeof(float));\n",
        "    cudaMalloc(&d_dz3, output_size * sizeof(float));\n",
        "\n",
        "    cudaMalloc(&d_loss, sizeof(float));\n",
        "    cudaMalloc(&d_predicted_class, sizeof(int));\n",
        "\n",
        "    // Copy data to device\n",
        "    cudaMemcpy(d_input, host_input.data(), input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_weights, host_hidden1_weights.data(), input_size * hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden1_biases, host_hidden1_biases.data(), hidden1_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_weights, host_hidden2_weights.data(), hidden1_size * hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden2_biases, host_hidden2_biases.data(), hidden2_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_weights, host_output_weights.data(), hidden2_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases.data(), output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_labels, host_labels.data(), output_size * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int num_samples = 60000; // Number of samples to process one by one\n",
        "    int num_epochs = 20; // Number of epochs\n",
        "    for (int epoch = 0; epoch < num_epochs; ++epoch) {\n",
        "    float total_loss = 0.0f; // Initialize total_loss at the start of each epoch\n",
        "    for (int sample = 0; sample < num_samples; ++sample) {\n",
        "        cudaMemcpy(d_input, host_input.data() + sample * input_size, input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_labels, host_labels.data() + sample * output_size, output_size * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Forward pass\n",
        "        linear_layer_and_activation_relu<<<(hidden1_size + 255) / 256, 256>>>(d_input, d_hidden1_weights, d_hidden1_biases, d_hidden1_output, input_size, hidden1_size);\n",
        "        linear_layer_and_activation_relu<<<(hidden2_size + 255) / 256, 256>>>(d_hidden1_output, d_hidden2_weights, d_hidden2_biases, d_hidden2_output, hidden1_size, hidden2_size);\n",
        "        linear_layer_and_activation<<<(output_size + 255) / 256, 256>>>(d_hidden2_output, d_output_weights, d_output_biases, d_output, hidden2_size, output_size);\n",
        "\n",
        "        // Apply softmax\n",
        "        softmax_layer<<<(output_size + 255) / 256, 256>>>(d_output, d_output, output_size);\n",
        "\n",
        "        // Compute cross-entropy loss\n",
        "        compute_cross_entropy_loss<<<1, 1>>>(d_output, d_labels, d_loss, output_size);\n",
        "\n",
        "        float sample_loss;\n",
        "        cudaMemcpy(&sample_loss, d_loss, sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Accumulate the loss for the current sample\n",
        "        total_loss += sample_loss;\n",
        "\n",
        "        // Backward pass\n",
        "        get_predicted_class<<<1, 1>>>(d_output, d_predicted_class, output_size);\n",
        "        cudaMemcpy(predicted_class, d_predicted_class, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        backpropagation_output<<<(output_size + 255) / 256, 256>>>(d_output, d_labels, d_dz3, d_dW3, d_db3, d_hidden2_output, hidden2_size, output_size);\n",
        "        backpropagation_hidden2<<<(hidden2_size + 255) / 256, 256>>>(d_dz3, d_output_weights, d_hidden2_output, d_dz2, d_dW2, d_db2, hidden1_size, hidden2_size);\n",
        "        backpropagation_hidden1<<<(hidden1_size + 255) / 256, 256>>>(d_dz2, d_hidden2_weights, d_input, d_dz1, d_dW1, d_db1, input_size, hidden1_size);\n",
        "\n",
        "        update_weights_biases<<<(input_size * hidden1_size + 255) / 256, 256>>>(\n",
        "            d_hidden1_weights, d_hidden1_biases,\n",
        "            d_hidden2_weights, d_hidden2_biases,\n",
        "            d_output_weights, d_output_biases,\n",
        "            d_dW1, d_db1, d_dW2, d_db2, d_dW3, d_db3,\n",
        "            learning_rate, input_size, hidden1_size, hidden2_size, output_size\n",
        "        );\n",
        "\n",
        "\n",
        "    }\n",
        "    float average_loss = total_loss / num_samples;\n",
        "    std::cout << \"Epoch \" << epoch + 1 << \" - Average Loss: \" << average_loss << std::endl;\n",
        "}\n",
        "\n",
        "    // Free memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden1_weights);\n",
        "    cudaFree(d_hidden1_biases);\n",
        "    cudaFree(d_hidden2_weights);\n",
        "    cudaFree(d_hidden2_biases);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_labels);\n",
        "    cudaFree(d_hidden1_output);\n",
        "    cudaFree(d_hidden2_output);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_dW1);\n",
        "    cudaFree(d_db1);\n",
        "    cudaFree(d_dW2);\n",
        "    cudaFree(d_db2);\n",
        "    cudaFree(d_dW3);\n",
        "    cudaFree(d_db3);\n",
        "    cudaFree(d_dz1);\n",
        "    cudaFree(d_dz2);\n",
        "    cudaFree(d_dz3);\n",
        "    cudaFree(d_loss);\n",
        "    cudaFree(d_predicted_class);\n",
        "    delete[] predicted_class;\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "Yk0osWxQL_bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a504f1f3-3ae6-4557-86e9-5abcb88588e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing back.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I/content/drive/MyDrive/Untitled_folder -o back back.cu\n",
        "!./back"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRqLtwkKE_Jv",
        "outputId": "33d80b0f-196e-4af1-d394-25d61ca4ce75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Average Loss: 1.38764\n",
            "Epoch 2 - Average Loss: 1.33207\n",
            "Epoch 3 - Average Loss: 1.33125\n",
            "Epoch 4 - Average Loss: 1.30591\n",
            "Epoch 5 - Average Loss: 1.31623\n",
            "Epoch 6 - Average Loss: 1.86744\n",
            "Epoch 7 - Average Loss: 1.28567\n",
            "Epoch 8 - Average Loss: 1.26964\n",
            "Epoch 9 - Average Loss: 3.19138\n",
            "Epoch 10 - Average Loss: 2.26431\n",
            "Epoch 11 - Average Loss: 1.51665\n",
            "Epoch 12 - Average Loss: 2.04775\n",
            "Epoch 13 - Average Loss: 5.37715\n",
            "Epoch 14 - Average Loss: 1.63338\n",
            "Epoch 15 - Average Loss: 1.34183\n",
            "Epoch 16 - Average Loss: 1.28342\n",
            "Epoch 17 - Average Loss: 1.30111\n",
            "Epoch 18 - Average Loss: 1.24131\n",
            "Epoch 19 - Average Loss: 1.174\n",
            "Epoch 20 - Average Loss: 1.93735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BACKWARD PROPAGATION ON COMPLETE DATASET(In batches)**"
      ],
      "metadata": {
        "id": "eDiSOASrgQxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile back_2.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <npy.hpp> // Include npy.hpp for loading .npy files\n",
        "\n",
        "#define INPUT_SIZE 784\n",
        "#define HIDDEN1_SIZE 512\n",
        "#define HIDDEN2_SIZE 256\n",
        "#define OUTPUT_SIZE 10\n",
        "#define BATCH_SIZE 128\n",
        "#define LEARNING_RATE 0.001f\n",
        "\n",
        "__device__ float relu(float x) {\n",
        "    return fmaxf(0.0f, x);\n",
        "}\n",
        "\n",
        "__device__ float relu_derivative(float x) {\n",
        "    return x > 0.0f ? 1.0f : 0.0f;\n",
        "}\n",
        "\n",
        "__device__ float softmax(float* output, int idx, int size) {\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        sum += expf(output[i]);\n",
        "    }\n",
        "    return expf(output[idx]) / sum;\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation_relu(float *input, float *weights, float *biases, float *output, int input_size, int output_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.y;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int batch_offset = batch_idx * input_size;\n",
        "\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[batch_offset + i];\n",
        "        }\n",
        "        output[batch_idx * output_size + idx] = relu(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.y;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int batch_offset = batch_idx * input_size;\n",
        "\n",
        "    if (idx < output_size) {\n",
        "        float z = biases[idx];\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[i * output_size + idx] * input[batch_offset + i];\n",
        "        }\n",
        "        output[batch_idx * output_size + idx] = z;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void softmax_layer(float *input, float *output, int size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int idx = threadIdx.x;\n",
        "\n",
        "    float *input_batch = input + batch_idx * size;\n",
        "    float *output_batch = output + batch_idx * size;\n",
        "\n",
        "    float max_val = input_batch[0];\n",
        "    for (int i = 1; i < size; ++i) {\n",
        "        if (input_batch[i] > max_val) max_val = input_batch[i];\n",
        "    }\n",
        "\n",
        "    float sum = 0.0f;\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output_batch[i] = expf(input_batch[i] - max_val);\n",
        "        sum += output_batch[i];\n",
        "    }\n",
        "\n",
        "    for (int i = 0; i < size; ++i) {\n",
        "        output_batch[i] /= sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void get_predicted_class(float *output, int *predicted_class, int output_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int idx = threadIdx.x;\n",
        "\n",
        "    float *output_batch = output + batch_idx * output_size;\n",
        "\n",
        "    if (idx == 0) {\n",
        "        int class_idx = 0;\n",
        "        float max_val = output_batch[0];\n",
        "        for (int i = 1; i < output_size; ++i) {\n",
        "            if (output_batch[i] > max_val) {\n",
        "                max_val = output_batch[i];\n",
        "                class_idx = i;\n",
        "            }\n",
        "        }\n",
        "        predicted_class[batch_idx] = class_idx;\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void compute_cross_entropy_loss(float *output, double *labels, float *loss, int size, int batch_size) {\n",
        "    int batch_idx = blockIdx.x;\n",
        "    int idx = threadIdx.x;\n",
        "\n",
        "    float *output_batch = output + batch_idx * size;\n",
        "    double *labels_batch = labels + batch_idx * size;\n",
        "\n",
        "    if (idx == 0) {\n",
        "        int true_class_idx = -1;\n",
        "        for (int i = 0; i < size; ++i) {\n",
        "            if (labels_batch[i] == 1.0) {\n",
        "                true_class_idx = i;\n",
        "                break;\n",
        "            }\n",
        "        }\n",
        "        if (true_class_idx != -1) {\n",
        "            float prob = output_batch[true_class_idx];\n",
        "            if (prob > 0) {\n",
        "                *loss = -logf(prob);\n",
        "            } else {\n",
        "                *loss = 0.0f;\n",
        "            }\n",
        "        } else {\n",
        "            *loss = 0.0f;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_output(float *d_output, double *d_labels, float *d_dz3, float *d_dW3, float *d_db3, float *d_a2, int hidden2_size, int output_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.y;\n",
        "    int output_idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (output_idx < output_size) {\n",
        "        d_dz3[batch_idx * output_size + output_idx] = d_output[batch_idx * output_size + output_idx] - d_labels[batch_idx * output_size + output_idx];\n",
        "        d_db3[output_idx] = d_dz3[batch_idx * output_size + output_idx];\n",
        "\n",
        "        for (int hidden_idx = 0; hidden_idx < hidden2_size; ++hidden_idx) {\n",
        "            d_dW3[hidden_idx * output_size + output_idx] += d_a2[batch_idx * hidden2_size + hidden_idx] * d_dz3[batch_idx * output_size + output_idx];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_hidden2(float *d_dz3, float *d_W3, float *d_a2, float *d_dz2, float *d_dW2, float *d_db2, int hidden1_size, int hidden2_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.y;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < hidden2_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < OUTPUT_SIZE; ++i) {\n",
        "            sum += d_dz3[batch_idx * OUTPUT_SIZE + i] * d_W3[idx * OUTPUT_SIZE + i];\n",
        "        }\n",
        "        d_dz2[batch_idx * hidden2_size + idx] = relu_derivative(d_a2[batch_idx * hidden2_size + idx]) * sum;\n",
        "\n",
        "        for (int i = 0; i < hidden1_size; ++i) {\n",
        "            d_dW2[idx * hidden1_size + i] += d_dz2[batch_idx * hidden2_size + idx] * d_a2[batch_idx * hidden1_size + i];\n",
        "        }\n",
        "        d_db2[idx] += d_dz2[batch_idx * hidden2_size + idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void backpropagation_hidden1(float *d_dz2, float *d_W2, float *d_x, float *d_dz1, float *d_dW1, float *d_db1, int input_size, int hidden1_size, int batch_size) {\n",
        "    int batch_idx = blockIdx.y;\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < hidden1_size) {\n",
        "        float sum = 0.0f;\n",
        "        for (int i = 0; i < HIDDEN2_SIZE; ++i) {\n",
        "            sum += d_dz2[batch_idx * HIDDEN2_SIZE + i] * d_W2[idx * HIDDEN2_SIZE + i];\n",
        "        }\n",
        "        d_dz1[batch_idx * hidden1_size + idx] = relu_derivative(d_x[batch_idx * input_size + idx]) * sum;\n",
        "        d_dW1[idx] += d_dz1[batch_idx * hidden1_size + idx] * d_x[batch_idx * input_size + idx];\n",
        "        d_db1[idx] += d_dz1[batch_idx * hidden1_size + idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void update_weights_biases(float *d_W1, float *d_b1, float *d_W2, float *d_b2, float *d_W3, float *d_b3,\n",
        "                                      float *d_dW1, float *d_db1, float *d_dW2, float *d_db2, float *d_dW3, float *d_db3,\n",
        "                                      float learning_rate, int input_size, int hidden1_size, int hidden2_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < input_size * hidden1_size) {\n",
        "        d_W1[idx] -= learning_rate * d_dW1[idx];\n",
        "    } else if (idx < input_size * hidden1_size + hidden1_size) {\n",
        "        d_b1[idx - input_size * hidden1_size] -= learning_rate * d_db1[idx - input_size * hidden1_size];\n",
        "    } else if (idx < input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size) {\n",
        "        d_W2[idx - (input_size * hidden1_size + hidden1_size)] -= learning_rate * d_dW2[idx - (input_size * hidden1_size + hidden1_size)];\n",
        "    } else if (idx < input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size + hidden2_size) {\n",
        "        d_b2[idx - (input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size)] -= learning_rate * d_db2[idx - (input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size)];\n",
        "    } else if (idx < input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size + hidden2_size * output_size) {\n",
        "        d_W3[idx - (input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size + hidden2_size)] -= learning_rate * d_dW3[idx - (input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size + hidden2_size)];\n",
        "    } else {\n",
        "        d_b3[idx - (input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size + hidden2_size * output_size)] -= learning_rate * d_db3[idx - (input_size * hidden1_size + hidden1_size + hidden1_size * hidden2_size + hidden2_size * output_size)];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Load dataset\n",
        "    std::vector<float> train_images;\n",
        "    std::vector<float> train_labels;\n",
        "\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\", {BATCH_SIZE, INPUT_SIZE}, train_images);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\", {BATCH_SIZE, OUTPUT_SIZE}, train_labels);\n",
        "\n",
        "    // Allocate and copy data to device\n",
        "    float *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, BATCH_SIZE * INPUT_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_output, BATCH_SIZE * OUTPUT_SIZE * sizeof(float));\n",
        "\n",
        "    // Load weights and biases from .npy files\n",
        "    float *d_weights1, *d_bias1;\n",
        "    float *d_weights2, *d_bias2;\n",
        "    float *d_weights3, *d_bias3;\n",
        "\n",
        "    cudaMalloc(&d_weights1, INPUT_SIZE * HIDDEN1_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_bias1, HIDDEN1_SIZE * sizeof(float));\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", {INPUT_SIZE, HIDDEN1_SIZE}, host_hidden1_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", {HIDDEN1_SIZE}, host_hidden1_biases);\n",
        "    cudaMemcpy(d_weights1, host_hidden1_weights.data(), INPUT_SIZE * HIDDEN1_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_bias1, host_hidden1_biases.data(), HIDDEN1_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Repeat for hidden2 and output layers\n",
        "    cudaMalloc(&d_weights2, HIDDEN1_SIZE * HIDDEN2_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_bias2, HIDDEN2_SIZE * sizeof(float));\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", {HIDDEN1_SIZE, HIDDEN2_SIZE}, host_hidden2_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", {HIDDEN2_SIZE}, host_hidden2_biases);\n",
        "    cudaMemcpy(d_weights2, host_hidden2_weights.data(), HIDDEN1_SIZE * HIDDEN2_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_bias2, host_hidden2_biases.data(), HIDDEN2_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    cudaMalloc(&d_weights3, HIDDEN2_SIZE * OUTPUT_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_bias3, OUTPUT_SIZE * sizeof(float));\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", {HIDDEN2_SIZE, OUTPUT_SIZE}, host_output_weights);\n",
        "    npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", {OUTPUT_SIZE}, host_output_biases);\n",
        "    cudaMemcpy(d_weights3, host_output_weights.data(), HIDDEN2_SIZE * OUTPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_bias3, host_output_biases.data(), OUTPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define other necessary variables and allocate memory for gradients\n",
        "    float *d_dW1, *d_dW2, *d_dW3;\n",
        "    float *d_db1, *d_db2, *d_db3;\n",
        "    cudaMalloc(&d_dW1, INPUT_SIZE * HIDDEN1_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_dW2, HIDDEN1_SIZE * HIDDEN2_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_dW3, HIDDEN2_SIZE * OUTPUT_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_db1, HIDDEN1_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_db2, HIDDEN2_SIZE * sizeof(float));\n",
        "    cudaMalloc(&d_db3, OUTPUT_SIZE * sizeof(float));\n",
        "\n",
        "    // Training loop\n",
        "    for (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {\n",
        "        for (int batch = 0; batch < NUM_BATCHES; ++batch) {\n",
        "            // Load data into d_input\n",
        "            cudaMemcpy(d_input, train_images.data() + batch * BATCH_SIZE * INPUT_SIZE, BATCH_SIZE * INPUT_SIZE * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "            // Forward pass\n",
        "            dim3 threadsPerBlock(256);\n",
        "            dim3 numBlocks1((HIDDEN1_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x, BATCH_SIZE);\n",
        "            linear_layer_and_activation_relu<<<numBlocks1, threadsPerBlock>>>(d_input, d_weights1, d_bias1, d_a1, INPUT_SIZE, HIDDEN1_SIZE, BATCH_SIZE);\n",
        "\n",
        "            dim3 numBlocks2((HIDDEN2_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x, BATCH_SIZE);\n",
        "            linear_layer_and_activation_relu<<<numBlocks2, threadsPerBlock>>>(d_a1, d_weights2, d_bias2, d_a2, HIDDEN1_SIZE, HIDDEN2_SIZE, BATCH_SIZE);\n",
        "\n",
        "            dim3 numBlocks3((OUTPUT_SIZE + threadsPerBlock.x - 1) / threadsPerBlock.x, BATCH_SIZE);\n",
        "            linear_layer_and_activation<<<numBlocks3, threadsPerBlock>>>(d_a2, d_weights3, d_bias3, d_output, HIDDEN2_SIZE, OUTPUT_SIZE, BATCH_SIZE);\n",
        "\n",
        "            // Softmax\n",
        "            dim3 numBlocksSoftmax(BATCH_SIZE);\n",
        "            softmax_layer<<<numBlocksSoftmax, OUTPUT_SIZE>>>(d_output, d_output, OUTPUT_SIZE, BATCH_SIZE);\n",
        "\n",
        "            // Compute loss\n",
        "            double *d_labels;\n",
        "            cudaMalloc(&d_labels, BATCH_SIZE * OUTPUT_SIZE * sizeof(double));\n",
        "            cudaMemcpy(d_labels, train_labels.data() + batch * BATCH_SIZE * OUTPUT_SIZE, BATCH_SIZE * OUTPUT_SIZE * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "            float *d_loss;\n",
        "            cudaMalloc(&d_loss, sizeof(float));\n",
        "            compute_cross_entropy_loss<<<BATCH_SIZE, 1>>>(d_output, d_labels, d_loss, OUTPUT_SIZE, BATCH_SIZE);\n",
        "\n",
        "            // Backpropagation\n",
        "            // Add your backward pass kernels here\n",
        "            // Example:\n",
        "            backpropagation_output<<<numBlocks3, threadsPerBlock>>>(d_output, d_labels, d_dz3, d_dW3, d_db3, d_a2, HIDDEN2_SIZE, OUTPUT_SIZE, BATCH_SIZE);\n",
        "            backpropagation_hidden2<<<numBlocks2, threadsPerBlock>>>(d_dz3, d_weights3, d_a2, d_dz2, d_dW2, d_db2, HIDDEN1_SIZE, HIDDEN2_SIZE, BATCH_SIZE);\n",
        "            backpropagation_hidden1<<<numBlocks1, threadsPerBlock>>>(d_dz2, d_weights2, d_input, d_dz1, d_dW1, d_db1, INPUT_SIZE, HIDDEN1_SIZE, BATCH_SIZE);\n",
        "\n",
        "            // Update weights and biases\n",
        "            int numWeights = INPUT_SIZE * HIDDEN1_SIZE + HIDDEN1_SIZE + HIDDEN1_SIZE * HIDDEN2_SIZE + HIDDEN2_SIZE + HIDDEN2_SIZE * OUTPUT_SIZE + OUTPUT_SIZE;\n",
        "            update_weights_biases<<<(numWeights + 255) / 256, 256>>>(d_weights1, d_bias1, d_weights2, d_bias2, d_weights3, d_bias3, d_dW1, d_db1, d_dW2, d_db2, d_dW3, d_db3, LEARNING_RATE, INPUT_SIZE, HIDDEN1_SIZE, HIDDEN2_SIZE, OUTPUT_SIZE);\n",
        "\n",
        "            // Free d_labels and d_loss\n",
        "            cudaFree(d_labels);\n",
        "            cudaFree(d_loss);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_weights1);\n",
        "    cudaFree(d_weights2);\n",
        "    cudaFree(d_weights3);\n",
        "    cudaFree(d_bias1);\n",
        "    cudaFree(d_bias2);\n",
        "    cudaFree(d_bias3);\n",
        "    cudaFree(d_dW1);\n",
        "    cudaFree(d_dW2);\n",
        "    cudaFree(d_dW3);\n",
        "    cudaFree(d_db1);\n",
        "    cudaFree(d_db2);\n",
        "    cudaFree(d_db3);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0lDuwzlfYoD",
        "outputId": "43e2c297-14b4-4060-d53c-38f7ff191e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting back_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -I/content/drive/MyDrive/Untitled_folder -o back_2 back_2.cu\n",
        "!./back_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFmhmbDQgy9X",
        "outputId": "422be1cc-80d8-43c6-8b78-71b0755f1224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(200)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [51], {...}, std::vector<float, std::allocator<float>>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\", {128, 784}, train_images);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(201)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [51], {...}, std::vector<float, std::allocator<float>>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\", {128, 10}, train_labels);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(215)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mhost_hidden1_weights\u001b[0m\" is undefined\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", {784, 512}, host_hidden1_weights);\n",
            "                                                                                                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(215)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [59], {...}, <error-type>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\", {784, 512}, host_hidden1_weights);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(216)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mhost_hidden1_biases\u001b[0m\" is undefined\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", {512}, host_hidden1_biases);\n",
            "                                                                                                  ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(216)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [58], {...}, <error-type>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\", {512}, host_hidden1_biases);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(223)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mhost_hidden2_weights\u001b[0m\" is undefined\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", {512, 256}, host_hidden2_weights);\n",
            "                                                                                                        ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(223)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [59], {...}, <error-type>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\", {512, 256}, host_hidden2_weights);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(224)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mhost_hidden2_biases\u001b[0m\" is undefined\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", {256}, host_hidden2_biases);\n",
            "                                                                                                  ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(224)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [58], {...}, <error-type>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\", {256}, host_hidden2_biases);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(230)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mhost_output_weights\u001b[0m\" is undefined\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", {256, 10}, host_output_weights);\n",
            "                                                                                                      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(230)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [58], {...}, <error-type>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\", {256, 10}, host_output_weights);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(231)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mhost_output_biases\u001b[0m\" is undefined\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", {10}, host_output_biases);\n",
            "                                                                                                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(231)\u001b[0m: \u001b[01;31merror\u001b[0m: no instance of overloaded function \u001b[01m\"npy::LoadArrayFromNumpy\"\u001b[0m matches the argument list\n",
            "            argument types are: (const char [57], {...}, <error-type>)\n",
            "      npy::LoadArrayFromNumpy(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\", {10}, host_output_biases);\n",
            "      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(246)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mNUM_EPOCHS\u001b[0m\" is undefined\n",
            "      for (int epoch = 0; epoch < NUM_EPOCHS; ++epoch) {\n",
            "                                  ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(247)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01mNUM_BATCHES\u001b[0m\" is undefined\n",
            "          for (int batch = 0; batch < NUM_BATCHES; ++batch) {\n",
            "                                      ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(254)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01md_a1\u001b[0m\" is undefined\n",
            "              linear_layer_and_activation_relu<<<numBlocks1, threadsPerBlock>>>(d_input, d_weights1, d_bias1, d_a1, 784, 512, 128);\n",
            "                                                                                                              ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(257)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01md_a2\u001b[0m\" is undefined\n",
            "              linear_layer_and_activation_relu<<<numBlocks2, threadsPerBlock>>>(d_a1, d_weights2, d_bias2, d_a2, 512, 256, 128);\n",
            "                                                                                                           ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(278)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01md_dz3\u001b[0m\" is undefined\n",
            "              backpropagation_output<<<numBlocks3, threadsPerBlock>>>(d_output, d_labels, d_dz3, d_dW3, d_db3, d_a2, 256, 10, 128);\n",
            "                                                                                          ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(279)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01md_dz2\u001b[0m\" is undefined\n",
            "              backpropagation_hidden2<<<numBlocks2, threadsPerBlock>>>(d_dz3, d_weights3, d_a2, d_dz2, d_dW2, d_db2, 512, 256, 128);\n",
            "                                                                                                ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01mback_2.cu(280)\u001b[0m: \u001b[01;31merror\u001b[0m: identifier \"\u001b[01md_dz1\u001b[0m\" is undefined\n",
            "              backpropagation_hidden1<<<numBlocks1, threadsPerBlock>>>(d_dz2, d_weights2, d_input, d_dz1, d_dW1, d_db1, 784, 512, 128);\n",
            "                                                                                                   ^\n",
            "\n",
            "21 errors detected in the compilation of \"back_2.cu\".\n",
            "/bin/bash: line 1: ./back_2: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "No2GSZNhs01U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eny60cDNmk6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}