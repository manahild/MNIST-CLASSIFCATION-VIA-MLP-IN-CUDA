{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "weNoqANX8Kpk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a94535-9c9f-435e-e12b-60f53fdd89b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data has been shuffled and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load the data from .npy files\n",
        "x_train = np.load(\"/content/drive/MyDrive/Untitled_folder/x_train.npy\")\n",
        "y_train = np.load(\"/content/drive/MyDrive/Untitled_folder/y_train.npy\")\n",
        "\n",
        "# Ensure the data is consistent\n",
        "assert len(x_train) == len(y_train), \"Mismatch in the number of images and labels\"\n",
        "\n",
        "# Generate a permutation of indices\n",
        "indices = np.arange(len(x_train))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "# Shuffle both x_train and y_train using the permutation of indices\n",
        "x_train_shuffled = x_train[indices]\n",
        "y_train_shuffled = y_train[indices]\n",
        "\n",
        "# Save the shuffled data to new .npy files\n",
        "np.save(\"/content/drive/MyDrive/Untitled_folder/x_train_shuffled.npy\", x_train_shuffled)\n",
        "np.save(\"/content/drive/MyDrive/Untitled_folder/y_train_shuffled.npy\", y_train_shuffled)\n",
        "\n",
        "print(\"Data has been shuffled and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KERAS IMPLEMENTATION**"
      ],
      "metadata": {
        "id": "-MTUNj3TAdNn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KVbu7uGCs5NX",
        "outputId": "e1ebb9c5-76ae-4853-a605-7f99cfd6e5b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">401,920</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │         \u001b[38;5;34m401,920\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m2,570\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">535,818</span> (2.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m535,818\u001b[0m (2.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">535,818</span> (2.04 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m535,818\u001b[0m (2.04 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            "469/469 - 6s - 13ms/step - accuracy: 0.7408 - loss: 1.1558\n",
            "Epoch 2/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.8831 - loss: 0.4578\n",
            "Epoch 3/35\n",
            "469/469 - 5s - 11ms/step - accuracy: 0.9006 - loss: 0.3629\n",
            "Epoch 4/35\n",
            "469/469 - 5s - 11ms/step - accuracy: 0.9101 - loss: 0.3212\n",
            "Epoch 5/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9169 - loss: 0.2946\n",
            "Epoch 6/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9226 - loss: 0.2747\n",
            "Epoch 7/35\n",
            "469/469 - 6s - 13ms/step - accuracy: 0.9272 - loss: 0.2584\n",
            "Epoch 8/35\n",
            "469/469 - 10s - 21ms/step - accuracy: 0.9308 - loss: 0.2445\n",
            "Epoch 9/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9343 - loss: 0.2323\n",
            "Epoch 10/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9377 - loss: 0.2214\n",
            "Epoch 11/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9404 - loss: 0.2115\n",
            "Epoch 12/35\n",
            "469/469 - 6s - 13ms/step - accuracy: 0.9432 - loss: 0.2025\n",
            "Epoch 13/35\n",
            "469/469 - 8s - 18ms/step - accuracy: 0.9455 - loss: 0.1942\n",
            "Epoch 14/35\n",
            "469/469 - 7s - 14ms/step - accuracy: 0.9475 - loss: 0.1866\n",
            "Epoch 15/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9495 - loss: 0.1794\n",
            "Epoch 16/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9516 - loss: 0.1728\n",
            "Epoch 17/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9536 - loss: 0.1666\n",
            "Epoch 18/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9554 - loss: 0.1608\n",
            "Epoch 19/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9568 - loss: 0.1554\n",
            "Epoch 20/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9579 - loss: 0.1503\n",
            "Epoch 21/35\n",
            "469/469 - 10s - 21ms/step - accuracy: 0.9594 - loss: 0.1455\n",
            "Epoch 22/35\n",
            "469/469 - 5s - 11ms/step - accuracy: 0.9611 - loss: 0.1409\n",
            "Epoch 23/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9622 - loss: 0.1366\n",
            "Epoch 24/35\n",
            "469/469 - 9s - 18ms/step - accuracy: 0.9635 - loss: 0.1325\n",
            "Epoch 25/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9646 - loss: 0.1286\n",
            "Epoch 26/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9654 - loss: 0.1249\n",
            "Epoch 27/35\n",
            "469/469 - 6s - 12ms/step - accuracy: 0.9664 - loss: 0.1214\n",
            "Epoch 28/35\n",
            "469/469 - 4s - 9ms/step - accuracy: 0.9676 - loss: 0.1180\n",
            "Epoch 29/35\n",
            "469/469 - 4s - 10ms/step - accuracy: 0.9685 - loss: 0.1148\n",
            "Epoch 30/35\n",
            "469/469 - 6s - 13ms/step - accuracy: 0.9696 - loss: 0.1117\n",
            "Epoch 31/35\n",
            "469/469 - 4s - 10ms/step - accuracy: 0.9704 - loss: 0.1088\n",
            "Epoch 32/35\n",
            "469/469 - 5s - 11ms/step - accuracy: 0.9713 - loss: 0.1059\n",
            "Epoch 33/35\n",
            "469/469 - 7s - 15ms/step - accuracy: 0.9719 - loss: 0.1032\n",
            "Epoch 34/35\n",
            "469/469 - 6s - 13ms/step - accuracy: 0.9727 - loss: 0.1006\n",
            "Epoch 35/35\n",
            "469/469 - 8s - 16ms/step - accuracy: 0.9735 - loss: 0.0981\n",
            "313/313 - 1s - 3ms/step - accuracy: 0.9670 - loss: 0.1102\n",
            "Test accuracy: 96.70%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(42)\n",
        "os.environ['PYTHONHASHSEED'] = str(42)\n",
        "\n",
        "# Additional configuration for reproducibility\n",
        "# This can vary based on the TensorFlow version and hardware, but it's good practice to include\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# 1. Load and Preprocess the Data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to a pixel value range of 0 to 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 2. Build the MLP Model\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(28, 28)))  # Input layer\n",
        "model.add(Flatten())  # Flatten the 28x28 images to a 1D vector\n",
        "model.add(Dense(512, activation='relu'))  # Fully connected layer with 512 units\n",
        "model.add(Dense(256, activation='relu'))  # Fully connected layer with 256 units\n",
        "model.add(Dense(10, activation='softmax'))  # Output layer with 10 units (one for each class)\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Save initial weights\n",
        "initial_weights = model.get_weights()\n",
        "np.savez('initial_weights.npz', *initial_weights)\n",
        "\n",
        "# 3. Compile the Model\n",
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the Model\n",
        "history = model.fit(x_train, y_train, epochs=35, batch_size=128, verbose=2)\n",
        "\n",
        "# Save trained weights\n",
        "trained_weights = model.get_weights()\n",
        "np.savez('trained_weights.npz', *trained_weights)\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('mnist_MLP.keras')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cqOgS1m81nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS FOR FIRST TRAINING EXAMPLE**"
      ],
      "metadata": {
        "id": "3ksQSo8_AmYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT5seAvgmPcS",
        "outputId": "18298270-ec76-4976-aa3a-dba3b9f42378"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weights of the first hidden layer (W1):\n",
            "[[ 0.04874337  0.02213094 -0.01922703 ...  0.06163257 -0.03144919\n",
            "   0.0341397 ]\n",
            " [ 0.02751227  0.02611049 -0.00524846 ...  0.03215915 -0.03102985\n",
            "  -0.00401989]\n",
            " [-0.01754742  0.05107565 -0.00734604 ...  0.03353318 -0.01523611\n",
            "   0.02394588]\n",
            " ...\n",
            " [ 0.02318398  0.01186018  0.00903605 ...  0.03019822  0.02065135\n",
            "  -0.05716718]\n",
            " [ 0.05829592  0.06271122  0.00899053 ...  0.01005762 -0.01094808\n",
            "  -0.01063794]\n",
            " [-0.01252478 -0.05149334 -0.05023279 ...  0.0500096  -0.00848199\n",
            "  -0.03685535]]\n",
            "Flattened input image: [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
            " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
            " 0.96862745 0.49803922 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
            " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.19215687\n",
            " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
            " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
            " 0.96862745 0.94509804 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
            " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.04313726\n",
            " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.13725491 0.94509804\n",
            " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
            " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
            " 0.5882353  0.10588235 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
            " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.15294118 0.5803922\n",
            " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
            " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.07058824 0.67058825\n",
            " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
            " 0.3137255  0.03529412 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
            " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.53333336 0.99215686\n",
            " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "z1 (pre-activation output of the first layer): [-2.40114946e-02 -3.02516103e-01  6.77785993e-01  4.62792397e-01\n",
            "  1.06064409e-01  3.37590754e-01 -1.47783011e-01  5.64984195e-02\n",
            "  6.12377107e-01  4.51602995e-01  4.47689533e-01  4.92754579e-01\n",
            " -3.03164452e-01  1.03062868e-01  1.50690153e-01 -2.89974451e-01\n",
            "  1.08023159e-01 -1.65032074e-01 -3.77614051e-02  8.05858672e-02\n",
            " -1.97949588e-01 -4.54427332e-01  8.17899823e-01  3.99620146e-01\n",
            " -3.99150819e-01 -4.58734147e-02 -3.92059982e-01  6.68435395e-01\n",
            " -5.10719359e-01  2.78849691e-01  3.98304164e-01  1.26700103e-01\n",
            " -2.21593842e-01  3.81794244e-01 -2.88603544e-01  4.50677067e-01\n",
            "  1.31126553e-01  2.71871388e-02  3.18950415e-01 -3.24573308e-01\n",
            "  1.56341881e-01  2.17970535e-01  1.05757549e-01 -3.26109827e-01\n",
            " -1.09031253e-01 -6.19389117e-01  8.07281025e-03  2.62251526e-01\n",
            "  2.75402009e-01  4.30476755e-01 -3.01196575e-01 -4.13244069e-01\n",
            " -8.95386994e-01  5.24340212e-01 -1.05078712e-01 -2.62123793e-02\n",
            "  2.13726372e-01 -5.62550545e-01 -1.39966056e-01  6.97115541e-01\n",
            " -6.79022908e-01  4.34297293e-01 -4.78490651e-01  3.41298342e-01\n",
            " -8.66652071e-01  5.34473397e-02 -3.70274216e-01  2.05504492e-01\n",
            " -2.85379499e-01  5.49563646e-01 -3.17762792e-01 -1.62723184e-01\n",
            " -4.86533642e-02 -7.69520581e-01  3.27153683e-01 -8.70212913e-02\n",
            " -8.22096244e-02 -2.98563927e-01 -2.50412017e-01 -3.58755112e-01\n",
            "  3.69229287e-01  2.39324719e-01 -1.20244265e+00 -6.10072255e-01\n",
            " -5.56182861e-02  2.56060779e-01  2.83041298e-01 -2.34104961e-01\n",
            "  8.17217708e-01  5.06656706e-01  3.39825541e-01  1.55122355e-01\n",
            " -3.17795351e-02 -1.89229801e-01  6.89761817e-01  1.21640041e-01\n",
            "  1.37272179e-01 -3.32715929e-01 -1.60782188e-02  4.43147242e-01\n",
            "  8.50617886e-01  5.59442103e-01  4.24251199e-01 -1.16814628e-01\n",
            "  2.72853553e-01  1.79000780e-01  3.75335604e-01  2.47058406e-01\n",
            " -5.39963365e-01 -4.21866894e-01 -1.72442973e-01 -3.17455292e-01\n",
            "  2.63092250e-01  2.72805870e-01  4.07701619e-02  2.23902583e-01\n",
            " -3.51875573e-01  9.40152444e-04 -6.57978117e-01 -4.27251935e-01\n",
            " -1.85694918e-03 -2.56933302e-01  6.89782351e-02  7.55645394e-01\n",
            "  2.22472586e-02 -6.52105093e-01  3.93461943e-01  7.64676183e-02\n",
            "  3.76885563e-01  2.68210500e-01 -3.96995842e-01  6.21980250e-01\n",
            " -1.46199763e-01 -2.15634882e-01 -3.96831930e-02  1.79982364e-01\n",
            "  8.50736141e-01 -1.58588380e-01 -2.69371271e-01 -4.49944139e-02\n",
            "  3.35764259e-01  2.51619041e-01 -7.37418115e-01  1.40354618e-01\n",
            " -1.41236424e-01  2.83180065e-02 -9.14940089e-02 -6.27812684e-01\n",
            " -2.96295583e-01  2.13360652e-01  2.19637468e-01 -3.25379550e-01\n",
            " -8.58750492e-02 -3.61242518e-02 -3.73545557e-01  3.00871074e-01\n",
            " -3.81735295e-01 -4.19762254e-01  3.86897624e-01 -4.59302276e-01\n",
            "  1.05751075e-01 -1.11706719e-01  7.63062239e-01 -2.21157134e-01\n",
            "  1.11102149e-01  4.61641699e-01 -1.99840263e-01  1.22123435e-01\n",
            " -4.05991137e-01  3.42279196e-01 -2.60696709e-01 -3.22999239e-01\n",
            "  4.83505458e-01 -7.00096905e-01 -3.92607957e-01 -3.05634230e-01\n",
            " -8.37003216e-02 -3.55080776e-02 -3.78415465e-01  4.65695053e-01\n",
            "  6.15390658e-01 -7.50573397e-01  5.30257933e-02  6.66549131e-02\n",
            "  5.92865646e-02  5.19135535e-01 -4.68066245e-01 -2.11980462e-01\n",
            "  4.19775814e-01 -3.37986261e-01 -4.84019607e-01  2.13939741e-01\n",
            "  4.02686179e-01 -2.34583974e-01 -4.09125030e-01  2.87608504e-01\n",
            "  5.23413271e-02  3.77203137e-01 -2.79511422e-01 -4.78820890e-01\n",
            " -4.68656331e-01  2.18390614e-01  2.28314325e-02 -3.45841795e-02\n",
            "  6.77575171e-01  3.53656560e-01 -1.79196924e-01  8.34726542e-02\n",
            " -4.56589870e-02 -6.67394757e-01 -7.06985950e-01 -3.10666561e-01\n",
            "  2.53643468e-02  5.63763529e-02 -1.41941056e-01  2.05309600e-01\n",
            " -2.07407579e-01 -1.75234407e-01 -4.51115638e-01  1.77916139e-01\n",
            "  2.89674342e-01 -5.54999173e-01  1.28437988e-02 -7.01476857e-02\n",
            " -9.55391973e-02 -2.54869699e-01 -2.37459674e-01 -2.09563479e-01\n",
            " -2.70228356e-01 -3.47334035e-02 -9.84603688e-02 -4.75654379e-02\n",
            "  2.49792904e-01 -2.67136283e-02 -3.24057043e-01  3.19096029e-01\n",
            " -5.34000278e-01  6.64570987e-01 -4.60707128e-01 -1.94644719e-01\n",
            " -1.72787786e-01 -1.64989129e-01  5.26469767e-01  1.76481642e-02\n",
            "  6.96070731e-01 -8.38731900e-02 -3.04081384e-02 -3.91534597e-01\n",
            " -1.35458991e-01  3.52236688e-01  3.39192301e-01 -3.01498592e-01\n",
            "  2.53071219e-01 -2.79821068e-01  1.64812014e-01 -5.56155778e-02\n",
            "  4.86618280e-03  2.00961336e-01  4.19369712e-03 -5.08014798e-01\n",
            " -1.45221651e-01 -2.48374715e-01  2.04763249e-01 -2.52357185e-01\n",
            " -5.68354666e-01 -1.11564368e-01  3.67649645e-01  4.66787815e-01\n",
            " -5.39043546e-01  7.13374674e-01  2.16557324e-01  1.43888980e-01\n",
            "  5.37436724e-01  2.75105149e-01 -2.69892931e-01 -1.10218134e-02\n",
            "  1.51395276e-01  2.49845758e-01  1.70224935e-01 -6.63513094e-02\n",
            "  3.19453299e-01  5.20095885e-01  6.85031474e-01 -7.16335773e-01\n",
            "  4.97636944e-02 -1.50917083e-01  8.14442158e-01 -3.82938087e-01\n",
            "  4.94984150e-01  2.34231055e-02 -5.78603864e-01 -3.39335427e-02\n",
            "  2.51154870e-01 -2.52569556e-01  7.26431727e-01 -2.25328714e-01\n",
            "  4.60579693e-01 -7.97554672e-01 -1.67089254e-01  8.79767060e-01\n",
            "  3.62306714e-01 -1.18701011e-01  3.16114932e-01 -2.41059080e-01\n",
            "  6.43092468e-02 -8.75070840e-02 -9.50031132e-02 -1.64932590e-02\n",
            " -5.04730463e-01  5.06772518e-01 -5.75659931e-01  2.56713666e-03\n",
            "  3.24971408e-01 -1.12228930e-01  4.80562091e-01 -1.78835317e-01\n",
            "  1.05152875e-01 -4.12828743e-01 -4.83123124e-01 -3.14984858e-01\n",
            " -4.75212276e-01  4.19607788e-01  6.87007964e-01 -3.62516232e-02\n",
            " -2.31431842e-01  4.31539625e-01  6.87048972e-01  4.61858839e-01\n",
            " -5.30869544e-01 -6.20452642e-01 -6.98340312e-03  4.06642824e-01\n",
            "  1.13312408e-01 -4.30627078e-01  3.85481000e-01 -5.48050642e-01\n",
            "  2.52457589e-01  1.64616674e-01 -1.16074361e-01  5.02323806e-01\n",
            " -4.62556124e-01 -9.19280350e-02 -6.30512238e-01  1.11118920e-01\n",
            "  1.23886466e-01 -2.48289317e-01  2.20076442e-01  6.52731732e-02\n",
            " -6.17053926e-01  5.41349389e-02 -4.21562850e-01  3.27139735e-01\n",
            " -2.25320727e-01 -3.73433053e-01  7.56722450e-01  7.46744215e-01\n",
            "  8.12983811e-01 -4.26152587e-01  2.92640030e-01 -3.50152105e-01\n",
            "  3.49295318e-01  3.49168390e-01 -5.60473263e-01  4.29261066e-02\n",
            "  2.20240027e-01  4.40246761e-01 -5.05963147e-01 -3.15376371e-01\n",
            "  4.29927737e-01 -4.94805351e-03 -3.26119959e-01  8.20565641e-01\n",
            " -8.93902838e-01  2.58624494e-01 -6.80320561e-02  7.91590691e-01\n",
            " -2.98486233e-01 -1.26650244e-01  3.44437003e-01  1.54697984e-01\n",
            "  1.06276177e-01  6.93664253e-01 -2.15330154e-01 -2.54038274e-01\n",
            " -2.46503264e-01 -4.24165130e-01 -4.35847938e-02  4.67146844e-01\n",
            " -2.15730250e-01 -3.30375999e-01 -1.67272136e-01 -1.18524790e-01\n",
            " -1.70554608e-01  6.63647950e-01  6.95801318e-01 -9.43991840e-02\n",
            "  8.75703931e-01  4.28993076e-01 -5.30907512e-01 -6.16007566e-01\n",
            " -3.32424909e-01 -5.34523949e-02 -1.68278098e-01  3.29640567e-01\n",
            " -6.84083998e-01 -1.42128050e-01 -7.31150091e-01  4.94532704e-01\n",
            " -3.82297993e-01  1.39768422e-01 -6.47098124e-01 -2.17231140e-01\n",
            " -2.90496111e-01 -6.73041523e-01 -3.79949421e-01  5.13448045e-02\n",
            " -1.57992944e-01 -2.83267349e-02  5.77754751e-02  1.66989058e-01\n",
            " -2.55007353e-02  1.15641039e-02  4.00462985e-01 -4.70840275e-01\n",
            " -2.44210705e-01 -5.87064981e-01  2.16126770e-01  2.17933774e-01\n",
            " -7.54323065e-01 -8.63772333e-02  3.03582489e-01 -2.18964368e-03\n",
            " -1.65077135e-01  2.35950455e-01 -3.82872224e-01  2.72097200e-01\n",
            "  1.11366475e+00  7.30041265e-01  1.07863195e-01  3.69229853e-01\n",
            " -8.54894280e-01 -2.17030197e-03  6.84509277e-01  3.01670879e-01\n",
            " -4.92382586e-01  3.82275254e-01  5.84530950e-01 -3.47876668e-01\n",
            " -3.25261891e-01  8.81519914e-02  3.28623027e-01 -1.17938288e-01\n",
            "  1.19940519e-01  4.01756525e-01 -4.25850958e-01 -6.79401532e-02\n",
            "  7.17956245e-01 -9.35904205e-01  1.57323077e-01 -8.01199302e-02\n",
            "  2.76950538e-01  2.32916966e-01  4.83665228e-01  2.82115787e-01\n",
            " -3.28439474e-01  6.38269857e-02 -2.28745975e-02  1.47827595e-01\n",
            "  2.06146210e-01  1.66860983e-01  2.43522730e-02  2.68384427e-01\n",
            " -4.10690367e-01  7.97430202e-02  3.85916144e-01 -5.88029325e-01\n",
            " -6.40508890e-01 -7.86842033e-02 -3.65151584e-01  1.89437225e-01\n",
            " -4.61366415e-01 -2.93724805e-01  2.27009922e-01 -1.98703527e-01\n",
            "  9.86795649e-02 -2.33460993e-01  9.34089899e-01  7.18846917e-02\n",
            " -2.78931856e-01 -4.00371194e-01 -8.60688090e-03  7.47809231e-01\n",
            " -1.86365545e-01 -1.99789882e-01 -2.79737152e-02 -1.58257455e-01\n",
            " -1.05884530e-01 -2.51733929e-01 -4.00892198e-02 -8.07068884e-01\n",
            "  1.57187939e-01  1.82803378e-01  6.09622449e-02 -4.00525033e-02\n",
            " -3.35172191e-02  1.73953936e-01 -3.53404164e-01 -3.18408668e-01\n",
            " -2.52346307e-01  9.55681443e-01  5.66359580e-01 -2.06987292e-01]\n",
            "a1 (activation output of the first layer): [0.00000000e+00 0.00000000e+00 6.77785993e-01 4.62792397e-01\n",
            " 1.06064409e-01 3.37590754e-01 0.00000000e+00 5.64984195e-02\n",
            " 6.12377107e-01 4.51602995e-01 4.47689533e-01 4.92754579e-01\n",
            " 0.00000000e+00 1.03062868e-01 1.50690153e-01 0.00000000e+00\n",
            " 1.08023159e-01 0.00000000e+00 0.00000000e+00 8.05858672e-02\n",
            " 0.00000000e+00 0.00000000e+00 8.17899823e-01 3.99620146e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.68435395e-01\n",
            " 0.00000000e+00 2.78849691e-01 3.98304164e-01 1.26700103e-01\n",
            " 0.00000000e+00 3.81794244e-01 0.00000000e+00 4.50677067e-01\n",
            " 1.31126553e-01 2.71871388e-02 3.18950415e-01 0.00000000e+00\n",
            " 1.56341881e-01 2.17970535e-01 1.05757549e-01 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 8.07281025e-03 2.62251526e-01\n",
            " 2.75402009e-01 4.30476755e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 5.24340212e-01 0.00000000e+00 0.00000000e+00\n",
            " 2.13726372e-01 0.00000000e+00 0.00000000e+00 6.97115541e-01\n",
            " 0.00000000e+00 4.34297293e-01 0.00000000e+00 3.41298342e-01\n",
            " 0.00000000e+00 5.34473397e-02 0.00000000e+00 2.05504492e-01\n",
            " 0.00000000e+00 5.49563646e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 3.27153683e-01 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 3.69229287e-01 2.39324719e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 2.56060779e-01 2.83041298e-01 0.00000000e+00\n",
            " 8.17217708e-01 5.06656706e-01 3.39825541e-01 1.55122355e-01\n",
            " 0.00000000e+00 0.00000000e+00 6.89761817e-01 1.21640041e-01\n",
            " 1.37272179e-01 0.00000000e+00 0.00000000e+00 4.43147242e-01\n",
            " 8.50617886e-01 5.59442103e-01 4.24251199e-01 0.00000000e+00\n",
            " 2.72853553e-01 1.79000780e-01 3.75335604e-01 2.47058406e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 2.63092250e-01 2.72805870e-01 4.07701619e-02 2.23902583e-01\n",
            " 0.00000000e+00 9.40152444e-04 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 6.89782351e-02 7.55645394e-01\n",
            " 2.22472586e-02 0.00000000e+00 3.93461943e-01 7.64676183e-02\n",
            " 3.76885563e-01 2.68210500e-01 0.00000000e+00 6.21980250e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.79982364e-01\n",
            " 8.50736141e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 3.35764259e-01 2.51619041e-01 0.00000000e+00 1.40354618e-01\n",
            " 0.00000000e+00 2.83180065e-02 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 2.13360652e-01 2.19637468e-01 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.00871074e-01\n",
            " 0.00000000e+00 0.00000000e+00 3.86897624e-01 0.00000000e+00\n",
            " 1.05751075e-01 0.00000000e+00 7.63062239e-01 0.00000000e+00\n",
            " 1.11102149e-01 4.61641699e-01 0.00000000e+00 1.22123435e-01\n",
            " 0.00000000e+00 3.42279196e-01 0.00000000e+00 0.00000000e+00\n",
            " 4.83505458e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65695053e-01\n",
            " 6.15390658e-01 0.00000000e+00 5.30257933e-02 6.66549131e-02\n",
            " 5.92865646e-02 5.19135535e-01 0.00000000e+00 0.00000000e+00\n",
            " 4.19775814e-01 0.00000000e+00 0.00000000e+00 2.13939741e-01\n",
            " 4.02686179e-01 0.00000000e+00 0.00000000e+00 2.87608504e-01\n",
            " 5.23413271e-02 3.77203137e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 2.18390614e-01 2.28314325e-02 0.00000000e+00\n",
            " 6.77575171e-01 3.53656560e-01 0.00000000e+00 8.34726542e-02\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 2.53643468e-02 5.63763529e-02 0.00000000e+00 2.05309600e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.77916139e-01\n",
            " 2.89674342e-01 0.00000000e+00 1.28437988e-02 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 2.49792904e-01 0.00000000e+00 0.00000000e+00 3.19096029e-01\n",
            " 0.00000000e+00 6.64570987e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 5.26469767e-01 1.76481642e-02\n",
            " 6.96070731e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 3.52236688e-01 3.39192301e-01 0.00000000e+00\n",
            " 2.53071219e-01 0.00000000e+00 1.64812014e-01 0.00000000e+00\n",
            " 4.86618280e-03 2.00961336e-01 4.19369712e-03 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 2.04763249e-01 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 3.67649645e-01 4.66787815e-01\n",
            " 0.00000000e+00 7.13374674e-01 2.16557324e-01 1.43888980e-01\n",
            " 5.37436724e-01 2.75105149e-01 0.00000000e+00 0.00000000e+00\n",
            " 1.51395276e-01 2.49845758e-01 1.70224935e-01 0.00000000e+00\n",
            " 3.19453299e-01 5.20095885e-01 6.85031474e-01 0.00000000e+00\n",
            " 4.97636944e-02 0.00000000e+00 8.14442158e-01 0.00000000e+00\n",
            " 4.94984150e-01 2.34231055e-02 0.00000000e+00 0.00000000e+00\n",
            " 2.51154870e-01 0.00000000e+00 7.26431727e-01 0.00000000e+00\n",
            " 4.60579693e-01 0.00000000e+00 0.00000000e+00 8.79767060e-01\n",
            " 3.62306714e-01 0.00000000e+00 3.16114932e-01 0.00000000e+00\n",
            " 6.43092468e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 5.06772518e-01 0.00000000e+00 2.56713666e-03\n",
            " 3.24971408e-01 0.00000000e+00 4.80562091e-01 0.00000000e+00\n",
            " 1.05152875e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 4.19607788e-01 6.87007964e-01 0.00000000e+00\n",
            " 0.00000000e+00 4.31539625e-01 6.87048972e-01 4.61858839e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.06642824e-01\n",
            " 1.13312408e-01 0.00000000e+00 3.85481000e-01 0.00000000e+00\n",
            " 2.52457589e-01 1.64616674e-01 0.00000000e+00 5.02323806e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.11118920e-01\n",
            " 1.23886466e-01 0.00000000e+00 2.20076442e-01 6.52731732e-02\n",
            " 0.00000000e+00 5.41349389e-02 0.00000000e+00 3.27139735e-01\n",
            " 0.00000000e+00 0.00000000e+00 7.56722450e-01 7.46744215e-01\n",
            " 8.12983811e-01 0.00000000e+00 2.92640030e-01 0.00000000e+00\n",
            " 3.49295318e-01 3.49168390e-01 0.00000000e+00 4.29261066e-02\n",
            " 2.20240027e-01 4.40246761e-01 0.00000000e+00 0.00000000e+00\n",
            " 4.29927737e-01 0.00000000e+00 0.00000000e+00 8.20565641e-01\n",
            " 0.00000000e+00 2.58624494e-01 0.00000000e+00 7.91590691e-01\n",
            " 0.00000000e+00 0.00000000e+00 3.44437003e-01 1.54697984e-01\n",
            " 1.06276177e-01 6.93664253e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.67146844e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 6.63647950e-01 6.95801318e-01 0.00000000e+00\n",
            " 8.75703931e-01 4.28993076e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.29640567e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 4.94532704e-01\n",
            " 0.00000000e+00 1.39768422e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.13448045e-02\n",
            " 0.00000000e+00 0.00000000e+00 5.77754751e-02 1.66989058e-01\n",
            " 0.00000000e+00 1.15641039e-02 4.00462985e-01 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 2.16126770e-01 2.17933774e-01\n",
            " 0.00000000e+00 0.00000000e+00 3.03582489e-01 0.00000000e+00\n",
            " 0.00000000e+00 2.35950455e-01 0.00000000e+00 2.72097200e-01\n",
            " 1.11366475e+00 7.30041265e-01 1.07863195e-01 3.69229853e-01\n",
            " 0.00000000e+00 0.00000000e+00 6.84509277e-01 3.01670879e-01\n",
            " 0.00000000e+00 3.82275254e-01 5.84530950e-01 0.00000000e+00\n",
            " 0.00000000e+00 8.81519914e-02 3.28623027e-01 0.00000000e+00\n",
            " 1.19940519e-01 4.01756525e-01 0.00000000e+00 0.00000000e+00\n",
            " 7.17956245e-01 0.00000000e+00 1.57323077e-01 0.00000000e+00\n",
            " 2.76950538e-01 2.32916966e-01 4.83665228e-01 2.82115787e-01\n",
            " 0.00000000e+00 6.38269857e-02 0.00000000e+00 1.47827595e-01\n",
            " 2.06146210e-01 1.66860983e-01 2.43522730e-02 2.68384427e-01\n",
            " 0.00000000e+00 7.97430202e-02 3.85916144e-01 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.89437225e-01\n",
            " 0.00000000e+00 0.00000000e+00 2.27009922e-01 0.00000000e+00\n",
            " 9.86795649e-02 0.00000000e+00 9.34089899e-01 7.18846917e-02\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.47809231e-01\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            " 1.57187939e-01 1.82803378e-01 6.09622449e-02 0.00000000e+00\n",
            " 0.00000000e+00 1.73953936e-01 0.00000000e+00 0.00000000e+00\n",
            " 0.00000000e+00 9.55681443e-01 5.66359580e-01 0.00000000e+00]\n",
            "z2 (pre-activation output of the second layer): [ 0.15504366 -0.3249679   0.53554773 -0.25955722  0.14329214 -0.00571838\n",
            " -0.61010796  0.48302233  0.23263565 -0.10553643  0.31707776 -0.00864637\n",
            " -0.00800005  0.18460065  0.65600175 -0.08710769  0.09941725 -0.23217517\n",
            " -0.51105803  0.4749604   0.2783048  -0.08281502 -0.2417431  -0.18354899\n",
            " -0.11708961 -0.6506161   0.04248001  0.3471605  -0.27809662 -0.47950166\n",
            "  0.07162737  0.2143386  -0.4355398  -0.7282439   0.32459596  0.28776672\n",
            " -0.10393746 -0.0925101   0.36365792 -0.26748776  0.08368263  0.04818258\n",
            "  0.46599558 -0.5292739  -0.00475276 -0.05077567 -0.28915346  0.04203008\n",
            "  0.12622266 -0.19542499  0.02369887 -0.6002309  -0.07284261 -0.27958804\n",
            " -0.5593276   0.8202893  -0.11300792 -0.09337512  0.3723554   0.12731807\n",
            " -0.08211713 -0.44755125 -0.38484278  0.09546894 -0.00936902 -0.31880093\n",
            "  0.17711094  0.04052483 -0.27443296 -0.93091327  0.11258248  0.5393645\n",
            " -0.21937354 -0.02501271 -0.23060186  0.24379596  0.00443516 -0.6166736\n",
            " -0.4386636  -0.16265464 -0.28040442  0.04275019  0.05522259 -0.03534356\n",
            " -0.17954178  0.21373609  0.18651989  0.46714953 -0.12046035 -0.03821187\n",
            " -0.01129328 -0.18543491 -0.45873058 -0.31300178 -0.13335918 -0.23540346\n",
            " -0.14154603  0.03379476 -0.46490267 -0.28109     0.13846242 -0.54544497\n",
            " -0.11028021 -0.21580821  0.10290372  0.00812224 -0.19623163  0.430033\n",
            "  0.11306027 -0.26001278  0.12658049  0.39783224  0.16427003  0.42814782\n",
            " -0.38660043  0.20140427 -0.276873    0.2652231  -0.37660477 -0.3290168\n",
            " -0.28973538  0.15244114 -0.17285192  0.39589044 -0.4004269   0.02664833\n",
            "  0.14902453  0.37637472 -0.35374564 -0.6539405  -0.35957667  0.02194985\n",
            " -0.883721   -0.59903014  0.5328708  -0.01675832 -0.36878765 -0.41837484\n",
            "  0.05741057 -0.10431834 -0.45917314  0.41131657 -0.16169892 -0.3655445\n",
            " -0.20209108  0.23709024  0.20850557  0.3247245   0.6030336  -0.5327724\n",
            "  0.21841611  0.19658789 -0.3003516   0.29565004  0.06210888  0.08731286\n",
            " -0.335906   -0.17031124  0.09178048  0.411164   -0.42684925 -0.08818552\n",
            "  0.08038089 -0.1648068   0.13236992 -0.12836322  0.3308867  -0.19426094\n",
            " -0.18988705  0.0676342  -0.45473522 -0.01747463  0.42823008 -0.13831311\n",
            "  0.09363552 -0.02738251  0.2817975   0.278071   -0.14744198 -0.6439936\n",
            " -0.24825194 -0.5579156   0.4697535  -0.20966224 -0.55691165  0.01200554\n",
            "  0.3659389  -0.22071296  0.22095972  0.47396988 -0.04029181  0.37532347\n",
            " -0.19472104  0.10306641 -0.07147384  0.38287878 -0.1391887  -0.18730317\n",
            "  0.8734315  -0.08769824  0.6297554  -0.3180506   0.01008435  0.32422495\n",
            "  0.50639045 -0.2763748   0.40933117  0.2057589   0.8060957   0.07815558\n",
            " -0.13523453 -0.7741233  -0.4013766  -0.07233014  0.02728183  0.24596071\n",
            " -0.3543744   0.28043085  0.25172693  0.45065427  0.27182055 -0.08474424\n",
            " -0.24632317  0.22714572 -0.08977625  0.637555   -0.34278205  0.17875561\n",
            " -0.26957607  0.09837114  0.24848635  0.37980163  0.2139898  -0.07108884\n",
            " -0.09464869  0.12693965 -0.14171559  0.24108729  0.5630983  -0.11639958\n",
            "  0.2718147   0.08501519  0.09322587  0.04769653  0.23565371  0.38881966\n",
            " -0.45242658 -0.39895174 -0.59964556  0.11624755  0.02097561 -0.16984177\n",
            " -0.11418956 -0.09960453  0.15522656 -0.63570076]\n",
            "a2 (activation output of the second layer): [0.15504366 0.         0.53554773 0.         0.14329214 0.\n",
            " 0.         0.48302233 0.23263565 0.         0.31707776 0.\n",
            " 0.         0.18460065 0.65600175 0.         0.09941725 0.\n",
            " 0.         0.4749604  0.2783048  0.         0.         0.\n",
            " 0.         0.         0.04248001 0.3471605  0.         0.\n",
            " 0.07162737 0.2143386  0.         0.         0.32459596 0.28776672\n",
            " 0.         0.         0.36365792 0.         0.08368263 0.04818258\n",
            " 0.46599558 0.         0.         0.         0.         0.04203008\n",
            " 0.12622266 0.         0.02369887 0.         0.         0.\n",
            " 0.         0.8202893  0.         0.         0.3723554  0.12731807\n",
            " 0.         0.         0.         0.09546894 0.         0.\n",
            " 0.17711094 0.04052483 0.         0.         0.11258248 0.5393645\n",
            " 0.         0.         0.         0.24379596 0.00443516 0.\n",
            " 0.         0.         0.         0.04275019 0.05522259 0.\n",
            " 0.         0.21373609 0.18651989 0.46714953 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.03379476 0.         0.         0.13846242 0.\n",
            " 0.         0.         0.10290372 0.00812224 0.         0.430033\n",
            " 0.11306027 0.         0.12658049 0.39783224 0.16427003 0.42814782\n",
            " 0.         0.20140427 0.         0.2652231  0.         0.\n",
            " 0.         0.15244114 0.         0.39589044 0.         0.02664833\n",
            " 0.14902453 0.37637472 0.         0.         0.         0.02194985\n",
            " 0.         0.         0.5328708  0.         0.         0.\n",
            " 0.05741057 0.         0.         0.41131657 0.         0.\n",
            " 0.         0.23709024 0.20850557 0.3247245  0.6030336  0.\n",
            " 0.21841611 0.19658789 0.         0.29565004 0.06210888 0.08731286\n",
            " 0.         0.         0.09178048 0.411164   0.         0.\n",
            " 0.08038089 0.         0.13236992 0.         0.3308867  0.\n",
            " 0.         0.0676342  0.         0.         0.42823008 0.\n",
            " 0.09363552 0.         0.2817975  0.278071   0.         0.\n",
            " 0.         0.         0.4697535  0.         0.         0.01200554\n",
            " 0.3659389  0.         0.22095972 0.47396988 0.         0.37532347\n",
            " 0.         0.10306641 0.         0.38287878 0.         0.\n",
            " 0.8734315  0.         0.6297554  0.         0.01008435 0.32422495\n",
            " 0.50639045 0.         0.40933117 0.2057589  0.8060957  0.07815558\n",
            " 0.         0.         0.         0.         0.02728183 0.24596071\n",
            " 0.         0.28043085 0.25172693 0.45065427 0.27182055 0.\n",
            " 0.         0.22714572 0.         0.637555   0.         0.17875561\n",
            " 0.         0.09837114 0.24848635 0.37980163 0.2139898  0.\n",
            " 0.         0.12693965 0.         0.24108729 0.5630983  0.\n",
            " 0.2718147  0.08501519 0.09322587 0.04769653 0.23565371 0.38881966\n",
            " 0.         0.         0.         0.11624755 0.02097561 0.\n",
            " 0.         0.         0.15522656 0.        ]\n",
            "z3 (pre-activation output of the output layer): [ 0.14456904 -0.48398128 -0.01432642 -0.2913961   0.647781    0.29022175\n",
            " -0.16285121 -0.00578885  0.2540359  -0.37841716]\n",
            "train_predictions (softmax probabilities): [0.10931252 0.05830341 0.09325293 0.07068587 0.18080568 0.1264521\n",
            " 0.08038203 0.09405249 0.12195812 0.06479476]\n",
            "The predicted class for the first training image (with initial weights) is: 4\n",
            "The actual class for the first training image is: 5\n",
            "Cross-Entropy Loss for the first training image: 2.0678915977478027\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to a pixel value range of 0 to 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Load initial weights\n",
        "initial_weights = np.load('/content/drive/MyDrive/Untitled_folder/initial_weights.npz', allow_pickle=True)\n",
        "weights = [initial_weights[key] for key in initial_weights]\n",
        "\n",
        "# Extract weights and biases\n",
        "W1, b1 = weights[0], weights[1]\n",
        "W2, b2 = weights[2], weights[3]\n",
        "W3, b3 = weights[4], weights[5]\n",
        "\n",
        "# Print weights of the first hidden layer\n",
        "print(f\"Weights of the first hidden layer (W1):\\n{W1}\")\n",
        "\n",
        "# Flatten the first image in the training set\n",
        "first_train_image = x_train[0:1]  # Shape (1, 28, 28)\n",
        "first_train_image_flattened = first_train_image.flatten()\n",
        "print(f\"Flattened input image: {first_train_image_flattened}\")\n",
        "\n",
        "# Forward propagation\n",
        "\n",
        "# First layer (Dense 512 -> ReLU)\n",
        "z1 = np.dot(first_train_image_flattened, W1) + b1\n",
        "a1 = np.maximum(0, z1)  # ReLU activation\n",
        "print(f\"z1 (pre-activation output of the first layer): {z1}\")\n",
        "print(f\"a1 (activation output of the first layer): {a1}\")\n",
        "\n",
        "# Second layer (Dense 256 -> ReLU)\n",
        "z2 = np.dot(a1, W2) + b2\n",
        "a2 = np.maximum(0, z2)  # ReLU activation\n",
        "print(f\"z2 (pre-activation output of the second layer): {z2}\")\n",
        "print(f\"a2 (activation output of the second layer): {a2}\")\n",
        "\n",
        "# Output layer (Dense 10 -> Softmax)\n",
        "z3 = np.dot(a2, W3) + b3\n",
        "exp_scores = np.exp(z3)\n",
        "train_predictions = exp_scores / np.sum(exp_scores)  # Softmax activation\n",
        "print(f\"z3 (pre-activation output of the output layer): {z3}\")\n",
        "print(f\"train_predictions (softmax probabilities): {train_predictions}\")\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_train_class = np.argmax(train_predictions)\n",
        "print(f\"The predicted class for the first training image (with initial weights) is: {predicted_train_class}\")\n",
        "\n",
        "# Print the actual class (as an integer index)\n",
        "actual_class = np.argmax(y_train[0])\n",
        "print(f\"The actual class for the first training image is: {actual_class}\")\n",
        "\n",
        "# Compute the cross-entropy loss\n",
        "# Cross-Entropy Loss = -log(p[true_class])\n",
        "cross_entropy_loss = -np.log(train_predictions[actual_class])\n",
        "print(f\"Cross-Entropy Loss for the first training image: {cross_entropy_loss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS FOR FORWARD PROP IN A BATCH**"
      ],
      "metadata": {
        "id": "OtaOn9slAz50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWCcA-n75-vq",
        "outputId": "8a53dd86-7700-4654-ae67-117348ad797f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image 1:\n",
            "  Actual class: 5\n",
            "  Predicted class: 4\n",
            "  Cross-Entropy Loss: 2.0678915977478027\n",
            "\n",
            "Image 2:\n",
            "  Actual class: 0\n",
            "  Predicted class: 4\n",
            "  Cross-Entropy Loss: 2.614464282989502\n",
            "\n",
            "Image 3:\n",
            "  Actual class: 4\n",
            "  Predicted class: 4\n",
            "  Cross-Entropy Loss: 2.0395843982696533\n",
            "\n",
            "Image 4:\n",
            "  Actual class: 1\n",
            "  Predicted class: 5\n",
            "  Cross-Entropy Loss: 2.2978529930114746\n",
            "\n",
            "Image 5:\n",
            "  Actual class: 9\n",
            "  Predicted class: 4\n",
            "  Cross-Entropy Loss: 2.650764226913452\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the images to a pixel value range of 0 to 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Load weights and biases directly from .npy files\n",
        "W1 = np.load(\"/content/drive/MyDrive/Untitled_folder/hidden1_weights.npy\")\n",
        "b1 = np.load(\"/content/drive/MyDrive/Untitled_folder/hidden1_biases.npy\")\n",
        "W2 = np.load(\"/content/drive/MyDrive/Untitled_folder/hidden2_weights.npy\")\n",
        "b2 = np.load(\"/content/drive/MyDrive/Untitled_folder/hidden2_biases.npy\")\n",
        "W3 = np.load(\"/content/drive/MyDrive/Untitled_folder/output_weights.npy\")\n",
        "b3 = np.load(\"/content/drive/MyDrive/Untitled_folder/output_biases.npy\")\n",
        "\n",
        "# Forward propagation for the first 5 images\n",
        "for i in range(5):\n",
        "    # Flatten the image\n",
        "    image = x_train[i:i+1].flatten()\n",
        "\n",
        "    # First layer (Dense 512 -> ReLU)\n",
        "    z1 = np.dot(image, W1) + b1\n",
        "    a1 = np.maximum(0, z1)  # ReLU activation\n",
        "\n",
        "    # Second layer (Dense 256 -> ReLU)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = np.maximum(0, z2)  # ReLU activation\n",
        "\n",
        "    # Output layer (Dense 10 -> Softmax)\n",
        "    z3 = np.dot(a2, W3) + b3\n",
        "    exp_scores = np.exp(z3)\n",
        "    predictions = exp_scores / np.sum(exp_scores)  # Softmax activation\n",
        "\n",
        "    # Get the predicted class\n",
        "    predicted_class = np.argmax(predictions)\n",
        "\n",
        "    # Print the actual class (as an integer index)\n",
        "    actual_class = np.argmax(y_train[i])\n",
        "\n",
        "    # Compute the cross-entropy loss\n",
        "    cross_entropy_loss = -np.log(predictions[actual_class])\n",
        "\n",
        "    # Print results for the current image\n",
        "    print(f\"Image {i+1}:\")\n",
        "    print(f\"  Actual class: {actual_class}\")\n",
        "    print(f\"  Predicted class: {predicted_class}\")\n",
        "    print(f\"  Cross-Entropy Loss: {cross_entropy_loss}\\n\")\n"
      ]
    }
  ]
}