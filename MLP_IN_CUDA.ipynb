{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ommsLpvg7Wtz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forward Propagation**"
      ],
      "metadata": {
        "id": "kjENJkF3wAF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile forward_prop.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "// Define the activation function (sigmoid in this case)\n",
        "__device__ float sigmoid(float x) {\n",
        "    return 1.0f / (1.0f + expf(-x));\n",
        "}\n",
        "\n",
        "// Kernel for the linear combination and activation\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = 0.0f;\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z +=  input[i] * weights[idx * input_size + i];\n",
        "        }\n",
        "        z += biases[idx];\n",
        "        output[idx] = sigmoid(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the architecture\n",
        "    const int input_size = 2;\n",
        "    const int hidden_size = 2;\n",
        "    const int output_size = 1;\n",
        "\n",
        "    // Initialize input data, weights and biases\n",
        "    float host_input[input_size] = {9.0f, 9.0f}; // Example input\n",
        "    float host_hidden_weights[input_size * hidden_size] ={0.15f, 0.25f, 0.20f, 0.30f};\n",
        "    float host_hidden_biases[hidden_size] = {0.35f, 0.35f};\n",
        "    float host_output_weights[hidden_size * output_size] = {0.4f, 0.5f};\n",
        "    float host_output_biases[output_size] = {0.6f};\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_input, *d_hidden_weights, *d_hidden_biases, *d_hidden_output;\n",
        "    float *d_output_weights, *d_output_biases, *d_output_output;\n",
        "    cudaMalloc((void**)&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_weights, input_size * hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_biases, hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_output, hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_weights, hidden_size * output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_output, output_size * sizeof(float));\n",
        "\n",
        "    // Copy data to the device\n",
        "    cudaMemcpy(d_input, host_input, input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden_weights, host_hidden_weights, input_size * hidden_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden_biases, host_hidden_biases, hidden_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_weights, host_output_weights, hidden_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch the kernel for the hidden layer\n",
        "    linear_layer_and_activation<<<1, hidden_size>>>(d_input, d_hidden_weights, d_hidden_biases, d_hidden_output, input_size, hidden_size);\n",
        "    cudaDeviceSynchronize(); // Ensure the hidden layer computation is complete\n",
        "\n",
        "    // Launch the kernel for the output layer\n",
        "    linear_layer_and_activation<<<1, output_size>>>(d_hidden_output, d_output_weights, d_output_biases, d_output_output, hidden_size, output_size);\n",
        "    cudaDeviceSynchronize(); // Ensure the output layer computation is complete\n",
        "\n",
        "    // Copy the result back to the host\n",
        "    float host_output[output_size];\n",
        "    cudaMemcpy(host_output, d_output_output, output_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print the result\n",
        "    std::cout << \"Output: \" << host_output[0] << std::endl;\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden_weights);\n",
        "    cudaFree(d_hidden_biases);\n",
        "    cudaFree(d_hidden_output);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_output_output);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkgADDehd9G3",
        "outputId": "4d8b00a2-1ba2-45ce-8280-e9cf50e0eefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing forward_prop.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc forward_prop.cu -o forward_prop\n",
        "!./forward_prop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTndu-1Q1jJD",
        "outputId": "63f60103-3074-437d-ca2e-c67938146ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: 0.815862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnCPs1vxwE9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back Propagation**"
      ],
      "metadata": {
        "id": "RXwW4D6pwFp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile l.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "// Define the activation function (sigmoid in this case)\n",
        "__device__ float sigmoid(float x) {\n",
        "    return 1.0f / (1.0f + expf(-x));\n",
        "}\n",
        "\n",
        "// Define the derivative of the sigmoid function\n",
        "__device__ float sigmoid_derivative(float x) {\n",
        "    return x * (1.0f - x);\n",
        "}\n",
        "\n",
        "// Kernel for the linear combination and activation\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = 0.0f;\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[idx * input_size + i] * input[i];\n",
        "        }\n",
        "        z += biases[idx];\n",
        "        output[idx] = sigmoid(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the output layer delta using cross-entropy loss\n",
        "__global__ void compute_output_delta(float *output, float *target, float *delta, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float p = output[idx];\n",
        "        float y = target[idx];\n",
        "        delta[idx] = p - y;  // Derivative of cross-entropy loss with respect to the output\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the hidden layer delta\n",
        "__global__ void compute_hidden_delta(float *output_delta, float *weights, float *hidden_output, float *hidden_delta, int output_size, int hidden_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden_size) {\n",
        "        float error = 0.0f;\n",
        "        for (int i = 0; i < output_size; ++i) {\n",
        "            error += output_delta[i] * weights[i * hidden_size + idx];\n",
        "        }\n",
        "        hidden_delta[idx] = error * sigmoid_derivative(hidden_output[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for updating the weights and biases\n",
        "__global__ void update_weights_and_biases(float *weights, float *biases, float *delta, float *input, int input_size, int output_size, float learning_rate) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            weights[idx * input_size + i] -= learning_rate * delta[idx] * input[i];\n",
        "        }\n",
        "        biases[idx] -= learning_rate * delta[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the cross-entropy loss\n",
        "__global__ void compute_loss(float *output, float *target, float *loss, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float p = output[idx];\n",
        "        float y = target[idx];\n",
        "        float epsilon = 1e-10f; // To avoid log(0)\n",
        "        float log10e = 2.302585092994046f; // Precomputed log_e(10) for base 10 conversion\n",
        "        loss[idx] = - (y * logf(p + epsilon) / log10e + (1.0f - y) * logf(1.0f - p + epsilon) / log10e); // Cross-entropy loss\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the architecture\n",
        "    const int input_size = 2;    // Number of inputs\n",
        "    const int hidden_size = 2;   // Number of neurons in the hidden layer\n",
        "    const int output_size = 1;   // Number of outputs\n",
        "\n",
        "    // Initialize input data, weights, biases, and target output\n",
        "    float host_input[input_size] = {9.0f, 9.0f}; // Example input\n",
        "    float host_target[output_size] = {1.0f}; // Target output for training\n",
        "\n",
        "    // Initialize weights and biases with hard-coded values\n",
        "    float host_hidden_weights[input_size * hidden_size] = {0.15f, 0.25f, 0.20f, 0.30f}; // Size = input_size * hidden_size\n",
        "    float host_hidden_biases[hidden_size] = {0.35f, 0.35f}; // Size = hidden_size\n",
        "    float host_output_weights[hidden_size * output_size] = {0.40f, 0.50f}; // Size = hidden_size * output_size\n",
        "    float host_output_biases[output_size] = {0.60f}; // Size = output_size\n",
        "\n",
        "    // Training parameters\n",
        "    const int epochs = 100;\n",
        "    const float learning_rate = 0.01f;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_input, *d_hidden_weights, *d_hidden_biases, *d_hidden_output;\n",
        "    float *d_output_weights, *d_output_biases, *d_output_output;\n",
        "    float *d_output_delta, *d_hidden_delta, *d_target, *d_loss;\n",
        "    cudaMalloc((void**)&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_weights, input_size * hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_biases, hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_output, hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_weights, hidden_size * output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_output, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_delta, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_hidden_delta, hidden_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_target, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_loss, output_size * sizeof(float));\n",
        "\n",
        "    // Copy data to the device\n",
        "    cudaMemcpy(d_input, host_input, input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_target, host_target, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden_weights, host_hidden_weights, input_size * hidden_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_hidden_biases, host_hidden_biases, hidden_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_weights, host_output_weights, hidden_size * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Training loop\n",
        "    for (int epoch = 0; epoch < epochs; ++epoch) {\n",
        "        // Forward pass\n",
        "        linear_layer_and_activation<<<1, hidden_size>>>(d_input, d_hidden_weights, d_hidden_biases, d_hidden_output, input_size, hidden_size);\n",
        "        cudaDeviceSynchronize(); // Ensure the hidden layer computation is complete\n",
        "\n",
        "        linear_layer_and_activation<<<1, output_size>>>(d_hidden_output, d_output_weights, d_output_biases, d_output_output, hidden_size, output_size);\n",
        "        cudaDeviceSynchronize(); // Ensure the output layer computation is complete\n",
        "\n",
        "        // Compute loss using current weights and biases\n",
        "        compute_loss<<<1, output_size>>>(d_output_output, d_target, d_loss, output_size);\n",
        "        cudaDeviceSynchronize(); // Ensure the loss computation is complete\n",
        "\n",
        "        float host_loss[output_size];\n",
        "        cudaMemcpy(host_loss, d_loss, output_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        float total_loss = 0.0f;\n",
        "        for (int i = 0; i < output_size; ++i) {\n",
        "            total_loss += host_loss[i];\n",
        "        }\n",
        "\n",
        "\n",
        "        // Compute deltas\n",
        "        compute_output_delta<<<1, output_size>>>(d_output_output, d_target, d_output_delta, output_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        compute_hidden_delta<<<1, hidden_size>>>(d_output_delta, d_output_weights, d_hidden_output, d_hidden_delta, output_size, hidden_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Update weights and biases\n",
        "        update_weights_and_biases<<<1, output_size>>>(d_output_weights, d_output_biases, d_output_delta, d_hidden_output, hidden_size, output_size, learning_rate);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        update_weights_and_biases<<<1, hidden_size>>>(d_hidden_weights, d_hidden_biases, d_hidden_delta, d_input, input_size, hidden_size, learning_rate);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Recompute forward pass with updated weights and biases\n",
        "        linear_layer_and_activation<<<1, hidden_size>>>(d_input, d_hidden_weights, d_hidden_biases, d_hidden_output, input_size, hidden_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        linear_layer_and_activation<<<1, output_size>>>(d_hidden_output, d_output_weights, d_output_biases, d_output_output, hidden_size, output_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Recompute loss with updated weights and biases\n",
        "        compute_loss<<<1, output_size>>>(d_output_output, d_target, d_loss, output_size);\n",
        "        cudaDeviceSynchronize(); // Ensure the loss computation is complete\n",
        "\n",
        "        cudaMemcpy(host_loss, d_loss, output_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        total_loss = 0.0f;\n",
        "        for (int i = 0; i < output_size; ++i) {\n",
        "            total_loss += host_loss[i];\n",
        "        }\n",
        "\n",
        "        // Print the new loss after weight and bias update\n",
        "        std::cout << \"Epoch \" << epoch + 1 << \" -  Loss: \" << total_loss << std::endl;\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_hidden_weights);\n",
        "    cudaFree(d_hidden_biases);\n",
        "    cudaFree(d_hidden_output);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_output_output);\n",
        "    cudaFree(d_output_delta);\n",
        "    cudaFree(d_hidden_delta);\n",
        "    cudaFree(d_target);\n",
        "    cudaFree(d_loss);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mn_T7kn07tN",
        "outputId": "f10d92e8-ffbb-47f1-af09-dd76c78bd145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting l.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc l.cu -o l\n",
        "!./l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u82SlJ4pzJbj",
        "outputId": "c106d63a-66d7-44ec-fb85-4a89c2a8b9db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 -  Loss: 0.0879486\n",
            "Epoch 2 -  Loss: 0.0875177\n",
            "Epoch 3 -  Loss: 0.0870906\n",
            "Epoch 4 -  Loss: 0.0866673\n",
            "Epoch 5 -  Loss: 0.0862476\n",
            "Epoch 6 -  Loss: 0.0858316\n",
            "Epoch 7 -  Loss: 0.0854191\n",
            "Epoch 8 -  Loss: 0.0850103\n",
            "Epoch 9 -  Loss: 0.0846049\n",
            "Epoch 10 -  Loss: 0.084203\n",
            "Epoch 11 -  Loss: 0.0838046\n",
            "Epoch 12 -  Loss: 0.0834095\n",
            "Epoch 13 -  Loss: 0.0830177\n",
            "Epoch 14 -  Loss: 0.0826293\n",
            "Epoch 15 -  Loss: 0.0822442\n",
            "Epoch 16 -  Loss: 0.0818622\n",
            "Epoch 17 -  Loss: 0.0814836\n",
            "Epoch 18 -  Loss: 0.081108\n",
            "Epoch 19 -  Loss: 0.0807356\n",
            "Epoch 20 -  Loss: 0.0803663\n",
            "Epoch 21 -  Loss: 0.0799999\n",
            "Epoch 22 -  Loss: 0.0796367\n",
            "Epoch 23 -  Loss: 0.0792764\n",
            "Epoch 24 -  Loss: 0.078919\n",
            "Epoch 25 -  Loss: 0.0785646\n",
            "Epoch 26 -  Loss: 0.078213\n",
            "Epoch 27 -  Loss: 0.0778643\n",
            "Epoch 28 -  Loss: 0.0775184\n",
            "Epoch 29 -  Loss: 0.0771753\n",
            "Epoch 30 -  Loss: 0.0768349\n",
            "Epoch 31 -  Loss: 0.0764973\n",
            "Epoch 32 -  Loss: 0.0761624\n",
            "Epoch 33 -  Loss: 0.0758301\n",
            "Epoch 34 -  Loss: 0.0755004\n",
            "Epoch 35 -  Loss: 0.0751734\n",
            "Epoch 36 -  Loss: 0.0748489\n",
            "Epoch 37 -  Loss: 0.074527\n",
            "Epoch 38 -  Loss: 0.0742076\n",
            "Epoch 39 -  Loss: 0.0738907\n",
            "Epoch 40 -  Loss: 0.0735762\n",
            "Epoch 41 -  Loss: 0.0732641\n",
            "Epoch 42 -  Loss: 0.0729546\n",
            "Epoch 43 -  Loss: 0.0726473\n",
            "Epoch 44 -  Loss: 0.0723425\n",
            "Epoch 45 -  Loss: 0.0720399\n",
            "Epoch 46 -  Loss: 0.0717397\n",
            "Epoch 47 -  Loss: 0.0714418\n",
            "Epoch 48 -  Loss: 0.0711461\n",
            "Epoch 49 -  Loss: 0.0708526\n",
            "Epoch 50 -  Loss: 0.0705614\n",
            "Epoch 51 -  Loss: 0.0702724\n",
            "Epoch 52 -  Loss: 0.0699855\n",
            "Epoch 53 -  Loss: 0.0697007\n",
            "Epoch 54 -  Loss: 0.0694181\n",
            "Epoch 55 -  Loss: 0.0691376\n",
            "Epoch 56 -  Loss: 0.0688592\n",
            "Epoch 57 -  Loss: 0.0685828\n",
            "Epoch 58 -  Loss: 0.0683084\n",
            "Epoch 59 -  Loss: 0.0680361\n",
            "Epoch 60 -  Loss: 0.0677657\n",
            "Epoch 61 -  Loss: 0.0674974\n",
            "Epoch 62 -  Loss: 0.0672309\n",
            "Epoch 63 -  Loss: 0.0669664\n",
            "Epoch 64 -  Loss: 0.0667038\n",
            "Epoch 65 -  Loss: 0.0664431\n",
            "Epoch 66 -  Loss: 0.0661843\n",
            "Epoch 67 -  Loss: 0.0659273\n",
            "Epoch 68 -  Loss: 0.0656721\n",
            "Epoch 69 -  Loss: 0.0654187\n",
            "Epoch 70 -  Loss: 0.0651673\n",
            "Epoch 71 -  Loss: 0.0649175\n",
            "Epoch 72 -  Loss: 0.0646695\n",
            "Epoch 73 -  Loss: 0.0644233\n",
            "Epoch 74 -  Loss: 0.0641788\n",
            "Epoch 75 -  Loss: 0.063936\n",
            "Epoch 76 -  Loss: 0.0636948\n",
            "Epoch 77 -  Loss: 0.0634554\n",
            "Epoch 78 -  Loss: 0.0632176\n",
            "Epoch 79 -  Loss: 0.0629815\n",
            "Epoch 80 -  Loss: 0.062747\n",
            "Epoch 81 -  Loss: 0.0625141\n",
            "Epoch 82 -  Loss: 0.0622828\n",
            "Epoch 83 -  Loss: 0.0620531\n",
            "Epoch 84 -  Loss: 0.0618249\n",
            "Epoch 85 -  Loss: 0.0615983\n",
            "Epoch 86 -  Loss: 0.0613733\n",
            "Epoch 87 -  Loss: 0.0611497\n",
            "Epoch 88 -  Loss: 0.0609276\n",
            "Epoch 89 -  Loss: 0.0607071\n",
            "Epoch 90 -  Loss: 0.060488\n",
            "Epoch 91 -  Loss: 0.0602704\n",
            "Epoch 92 -  Loss: 0.0600543\n",
            "Epoch 93 -  Loss: 0.0598395\n",
            "Epoch 94 -  Loss: 0.0596262\n",
            "Epoch 95 -  Loss: 0.0594143\n",
            "Epoch 96 -  Loss: 0.0592038\n",
            "Epoch 97 -  Loss: 0.0589947\n",
            "Epoch 98 -  Loss: 0.058787\n",
            "Epoch 99 -  Loss: 0.0585806\n",
            "Epoch 100 -  Loss: 0.0583755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generic**"
      ],
      "metadata": {
        "id": "oybhhouknwok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back Propagation Generic**"
      ],
      "metadata": {
        "id": "vT40p70PwLIM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbcRYVST7KO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile values.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "#define MAX_LAYERS 10 // Define the maximum number of layers you want to support\n",
        "\n",
        "// Define the activation function (sigmoid in this case)\n",
        "__device__ float sigmoid(float x) {\n",
        "    return 1.0f / (1.0f + expf(-x));\n",
        "}\n",
        "\n",
        "// Define the derivative of the sigmoid function\n",
        "__device__ float sigmoid_derivative(float x) {\n",
        "    return x * (1.0f - x);\n",
        "}\n",
        "\n",
        "// Kernel for the linear combination and activation for any layer\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = 0.0f;\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[idx * input_size + i] * input[i];\n",
        "        }\n",
        "        z += biases[idx];\n",
        "        output[idx] = sigmoid(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the output layer delta using cross-entropy loss\n",
        "__global__ void compute_output_delta(float *output, float *target, float *delta, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float p = output[idx];\n",
        "        float y = target[idx];\n",
        "        delta[idx] = p - y;  // Derivative of cross-entropy loss with respect to the output\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the hidden layer delta\n",
        "__global__ void compute_hidden_delta(float *output_delta, float *weights, float *hidden_output, float *hidden_delta, int output_size, int hidden_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden_size) {\n",
        "        float error = 0.0f;\n",
        "        for (int i = 0; i < output_size; ++i) {\n",
        "            error += output_delta[i] * weights[i * hidden_size + idx];\n",
        "        }\n",
        "        hidden_delta[idx] = error * sigmoid_derivative(hidden_output[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for updating the weights and biases\n",
        "__global__ void update_weights_and_biases(float *weights, float *biases, float *delta, float *input, int input_size, int output_size, float learning_rate) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            weights[idx * input_size + i] -= learning_rate * delta[idx] * input[i];\n",
        "        }\n",
        "        biases[idx] -= learning_rate * delta[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the cross-entropy loss\n",
        "__global__ void compute_loss(float *output, float *target, float *loss, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float p = output[idx];\n",
        "        float y = target[idx];\n",
        "        float epsilon = 1e-10f; // To avoid log(0)\n",
        "        float log10e = 2.302585092994046f; // Precomputed log_e(10) for base 10 conversion\n",
        "        loss[idx] = - (y * logf(p + epsilon) / log10e + (1.0f - y) * logf(1.0f - p + epsilon) / log10e); // Cross-entropy loss\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the architecture\n",
        "    const int input_size = 2;    // Number of inputs\n",
        "    const int hidden_sizes[MAX_LAYERS] = {2};   // Number of neurons in each hidden layer\n",
        "    const int num_hidden_layers = 1; // Number of hidden layers\n",
        "    const int output_size = 1;   // Number of outputs\n",
        "\n",
        "    // Initialize input data, weights, biases, and target output\n",
        "    float host_input[input_size] = {9.0f, 9.0f}; // Example input\n",
        "    float host_target[output_size] = {1.0f}; // Target output for training\n",
        "\n",
        "    // Initialize weights and biases with hard-coded values\n",
        "    float host_weights[MAX_LAYERS][input_size * hidden_sizes[0]] = { {0.15f, 0.25f, 0.20f, 0.30f} }; // Example\n",
        "    float host_biases[MAX_LAYERS][hidden_sizes[0]] = { {0.35f, 0.35f} }; // Example\n",
        "    float host_output_weights[hidden_sizes[0] * output_size] = {0.40f, 0.50f}; // Example\n",
        "    float host_output_biases[output_size] = {0.60f}; // Example\n",
        "\n",
        "    // Training parameters\n",
        "    const int epochs = 100;\n",
        "    const float learning_rate = 0.01f;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_input, *d_weights[MAX_LAYERS], *d_biases[MAX_LAYERS], *d_hidden_output[MAX_LAYERS], *d_output_weights, *d_output_biases, *d_output_output;\n",
        "    float *d_output_delta, *d_hidden_delta[MAX_LAYERS], *d_target, *d_loss;\n",
        "\n",
        "    cudaMalloc((void**)&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_target, output_size * sizeof(float));\n",
        "\n",
        "    // Allocate memory for weights and biases of hidden layers\n",
        "    for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "        cudaMalloc((void**)&d_weights[l], (l == 0 ? input_size : hidden_sizes[l-1]) * hidden_sizes[l] * sizeof(float));\n",
        "        cudaMalloc((void**)&d_biases[l], hidden_sizes[l] * sizeof(float));\n",
        "        cudaMalloc((void**)&d_hidden_output[l], hidden_sizes[l] * sizeof(float));\n",
        "        cudaMalloc((void**)&d_hidden_delta[l], hidden_sizes[l] * sizeof(float));\n",
        "\n",
        "        cudaMemcpy(d_weights[l], host_weights[l], (l == 0 ? input_size : hidden_sizes[l-1]) * hidden_sizes[l] * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_biases[l], host_biases[l], hidden_sizes[l] * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    }\n",
        "    cudaMalloc((void**)&d_output_weights, hidden_sizes[num_hidden_layers-1] * output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_output, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_delta, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_loss, output_size * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_output_weights, host_output_weights, hidden_sizes[num_hidden_layers-1] * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Copy input data and target output to device\n",
        "    cudaMemcpy(d_input, host_input, input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_target, host_target, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Training loop\n",
        "    // Training loop\n",
        "for (int epoch = 0; epoch < epochs+1; ++epoch) {\n",
        "    // Forward pass\n",
        "    float *d_input_current = d_input;\n",
        "    for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "        linear_layer_and_activation<<<1, hidden_sizes[l]>>>(d_input_current, d_weights[l], d_biases[l], d_hidden_output[l], (l == 0 ? input_size : hidden_sizes[l-1]), hidden_sizes[l]);\n",
        "        cudaDeviceSynchronize();\n",
        "        d_input_current = d_hidden_output[l];\n",
        "    }\n",
        "\n",
        "    linear_layer_and_activation<<<1, output_size>>>(d_input_current, d_output_weights, d_output_biases, d_output_output, hidden_sizes[num_hidden_layers-1], output_size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Compute loss using current weights and biases\n",
        "    compute_loss<<<1, output_size>>>(d_output_output, d_target, d_loss, output_size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    float host_loss[output_size];\n",
        "    cudaMemcpy(host_loss, d_loss, output_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "    float total_loss = 0.0f;\n",
        "    for (int i = 0; i < output_size; ++i) {\n",
        "        total_loss += host_loss[i];\n",
        "    }\n",
        "    // Compute deltas\n",
        "    compute_output_delta<<<1, output_size>>>(d_output_output, d_target, d_output_delta, output_size);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    float *d_delta_current = d_output_delta;\n",
        "    for (int l = num_hidden_layers-1; l >= 0; --l) {\n",
        "        compute_hidden_delta<<<1, hidden_sizes[l]>>>(d_delta_current, d_weights[l], d_hidden_output[l], d_hidden_delta[l], hidden_sizes[l+1], hidden_sizes[l]);\n",
        "        cudaDeviceSynchronize();\n",
        "        d_delta_current = d_hidden_delta[l];\n",
        "    }\n",
        "\n",
        "    // Update weights and biases\n",
        "    float *d_input_for_update = d_input;\n",
        "    for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "        update_weights_and_biases<<<1, hidden_sizes[l]>>>(d_weights[l], d_biases[l], d_hidden_delta[l], d_input_for_update, (l == 0 ? input_size : hidden_sizes[l-1]), hidden_sizes[l], learning_rate);\n",
        "        cudaDeviceSynchronize();\n",
        "        d_input_for_update = d_hidden_output[l];\n",
        "    }\n",
        "\n",
        "    update_weights_and_biases<<<1, output_size>>>(d_output_weights, d_output_biases, d_output_delta, d_hidden_output[num_hidden_layers-1], hidden_sizes[num_hidden_layers-1], output_size, learning_rate);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Only print loss from epoch 1 onwards\n",
        "    if (epoch > 0) {\n",
        "        std::cout << \"Epoch \" << epoch << \" Loss: \" << total_loss << std::endl;\n",
        "    }\n",
        "}\n",
        "    // Clean up\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_target);\n",
        "    for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "        cudaFree(d_weights[l]);\n",
        "        cudaFree(d_biases[l]);\n",
        "        cudaFree(d_hidden_output[l]);\n",
        "        cudaFree(d_hidden_delta[l]);\n",
        "    }\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_output_output);\n",
        "    cudaFree(d_output_delta);\n",
        "    cudaFree(d_loss);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "qewpzuf9i6p5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50b2583-c450-4975-b854-fb23bab3f7ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing values.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc values.cu -o values\n",
        "!./values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqAX5sUzz7Uf",
        "outputId": "71297e51-a3e4-4b24-febb-b40a9e5283c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.0879503\n",
            "Epoch 2 Loss: 0.0875211\n",
            "Epoch 3 Loss: 0.0870956\n",
            "Epoch 4 Loss: 0.0866739\n",
            "Epoch 5 Loss: 0.0862559\n",
            "Epoch 6 Loss: 0.0858415\n",
            "Epoch 7 Loss: 0.0854307\n",
            "Epoch 8 Loss: 0.0850234\n",
            "Epoch 9 Loss: 0.0846197\n",
            "Epoch 10 Loss: 0.0842194\n",
            "Epoch 11 Loss: 0.0838225\n",
            "Epoch 12 Loss: 0.0834291\n",
            "Epoch 13 Loss: 0.0830389\n",
            "Epoch 14 Loss: 0.082652\n",
            "Epoch 15 Loss: 0.0822685\n",
            "Epoch 16 Loss: 0.0818881\n",
            "Epoch 17 Loss: 0.081511\n",
            "Epoch 18 Loss: 0.0811369\n",
            "Epoch 19 Loss: 0.080766\n",
            "Epoch 20 Loss: 0.0803982\n",
            "Epoch 21 Loss: 0.0800334\n",
            "Epoch 22 Loss: 0.0796716\n",
            "Epoch 23 Loss: 0.0793128\n",
            "Epoch 24 Loss: 0.0789569\n",
            "Epoch 25 Loss: 0.0786039\n",
            "Epoch 26 Loss: 0.0782539\n",
            "Epoch 27 Loss: 0.0779066\n",
            "Epoch 28 Loss: 0.0775621\n",
            "Epoch 29 Loss: 0.0772204\n",
            "Epoch 30 Loss: 0.0768815\n",
            "Epoch 31 Loss: 0.0765452\n",
            "Epoch 32 Loss: 0.0762117\n",
            "Epoch 33 Loss: 0.0758808\n",
            "Epoch 34 Loss: 0.0755526\n",
            "Epoch 35 Loss: 0.0752268\n",
            "Epoch 36 Loss: 0.0749037\n",
            "Epoch 37 Loss: 0.0745832\n",
            "Epoch 38 Loss: 0.074265\n",
            "Epoch 39 Loss: 0.0739495\n",
            "Epoch 40 Loss: 0.0736364\n",
            "Epoch 41 Loss: 0.0733256\n",
            "Epoch 42 Loss: 0.0730173\n",
            "Epoch 43 Loss: 0.0727114\n",
            "Epoch 44 Loss: 0.0724078\n",
            "Epoch 45 Loss: 0.0721065\n",
            "Epoch 46 Loss: 0.0718076\n",
            "Epoch 47 Loss: 0.0715109\n",
            "Epoch 48 Loss: 0.0712164\n",
            "Epoch 49 Loss: 0.0709243\n",
            "Epoch 50 Loss: 0.0706342\n",
            "Epoch 51 Loss: 0.0703464\n",
            "Epoch 52 Loss: 0.0700607\n",
            "Epoch 53 Loss: 0.0697771\n",
            "Epoch 54 Loss: 0.0694957\n",
            "Epoch 55 Loss: 0.0692163\n",
            "Epoch 56 Loss: 0.0689391\n",
            "Epoch 57 Loss: 0.0686638\n",
            "Epoch 58 Loss: 0.0683906\n",
            "Epoch 59 Loss: 0.0681194\n",
            "Epoch 60 Loss: 0.0678502\n",
            "Epoch 61 Loss: 0.0675829\n",
            "Epoch 62 Loss: 0.0673176\n",
            "Epoch 63 Loss: 0.0670542\n",
            "Epoch 64 Loss: 0.0667926\n",
            "Epoch 65 Loss: 0.066533\n",
            "Epoch 66 Loss: 0.0662753\n",
            "Epoch 67 Loss: 0.0660194\n",
            "Epoch 68 Loss: 0.0657653\n",
            "Epoch 69 Loss: 0.0655129\n",
            "Epoch 70 Loss: 0.0652625\n",
            "Epoch 71 Loss: 0.0650137\n",
            "Epoch 72 Loss: 0.0647668\n",
            "Epoch 73 Loss: 0.0645215\n",
            "Epoch 74 Loss: 0.064278\n",
            "Epoch 75 Loss: 0.0640362\n",
            "Epoch 76 Loss: 0.063796\n",
            "Epoch 77 Loss: 0.0635576\n",
            "Epoch 78 Loss: 0.0633208\n",
            "Epoch 79 Loss: 0.0630856\n",
            "Epoch 80 Loss: 0.062852\n",
            "Epoch 81 Loss: 0.0626201\n",
            "Epoch 82 Loss: 0.0623897\n",
            "Epoch 83 Loss: 0.0621609\n",
            "Epoch 84 Loss: 0.0619336\n",
            "Epoch 85 Loss: 0.0617079\n",
            "Epoch 86 Loss: 0.0614838\n",
            "Epoch 87 Loss: 0.0612611\n",
            "Epoch 88 Loss: 0.06104\n",
            "Epoch 89 Loss: 0.0608203\n",
            "Epoch 90 Loss: 0.0606021\n",
            "Epoch 91 Loss: 0.0603853\n",
            "Epoch 92 Loss: 0.06017\n",
            "Epoch 93 Loss: 0.0599561\n",
            "Epoch 94 Loss: 0.0597437\n",
            "Epoch 95 Loss: 0.0595326\n",
            "Epoch 96 Loss: 0.0593229\n",
            "Epoch 97 Loss: 0.0591147\n",
            "Epoch 98 Loss: 0.0589077\n",
            "Epoch 99 Loss: 0.0587021\n",
            "Epoch 100 Loss: 0.0584978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BkvIN26O0LOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Back prop 2 hidden layers**"
      ],
      "metadata": {
        "id": "7KPO-scsXSHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile back_prop_2.cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <device_launch_parameters.h>\n",
        "#include <iostream>\n",
        "#include <cmath>\n",
        "\n",
        "#define MAX_LAYERS 10 // Define the maximum number of layers you want to support\n",
        "\n",
        "// Define the activation function (sigmoid in this case)\n",
        "__device__ float sigmoid(float x) {\n",
        "    return 1.0f / (1.0f + expf(-x));\n",
        "}\n",
        "\n",
        "// Define the derivative of the sigmoid function\n",
        "__device__ float sigmoid_derivative(float x) {\n",
        "    return x * (1.0f - x);\n",
        "}\n",
        "\n",
        "// Kernel for the linear combination and activation for any layer\n",
        "__global__ void linear_layer_and_activation(float *input, float *weights, float *biases, float *output, int input_size, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float z = 0.0f;\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            z += weights[idx * input_size + i] * input[i];\n",
        "        }\n",
        "        z += biases[idx];\n",
        "        output[idx] = sigmoid(z);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the output layer delta using cross-entropy loss\n",
        "__global__ void compute_output_delta(float *output, float *target, float *delta, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float p = output[idx];\n",
        "        float y = target[idx];\n",
        "        delta[idx] = p - y;  // Derivative of cross-entropy loss with respect to the output\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the hidden layer delta\n",
        "__global__ void compute_hidden_delta(float *output_delta, float *weights, float *hidden_output, float *hidden_delta, int output_size, int hidden_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < hidden_size) {\n",
        "        float error = 0.0f;\n",
        "        for (int i = 0; i < output_size; ++i) {\n",
        "            error += output_delta[i] * weights[i * hidden_size + idx];\n",
        "        }\n",
        "        hidden_delta[idx] = error * sigmoid_derivative(hidden_output[idx]);\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for updating the weights and biases\n",
        "__global__ void update_weights_and_biases(float *weights, float *biases, float *delta, float *input, int input_size, int output_size, float learning_rate) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        for (int i = 0; i < input_size; ++i) {\n",
        "            weights[idx * input_size + i] -= learning_rate * delta[idx] * input[i];\n",
        "        }\n",
        "        biases[idx] -= learning_rate * delta[idx];\n",
        "    }\n",
        "}\n",
        "\n",
        "// Kernel for computing the cross-entropy loss\n",
        "__global__ void compute_loss(float *output, float *target, float *loss, int output_size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    if (idx < output_size) {\n",
        "        float p = output[idx];\n",
        "        float y = target[idx];\n",
        "        float epsilon = 1e-10f; // To avoid log(0)\n",
        "        float log10e = 2.302585092994046f; // Precomputed log_e(10) for base 10 conversion\n",
        "        loss[idx] = - (y * logf(p + epsilon) / log10e + (1.0f - y) * logf(1.0f - p + epsilon) / log10e); // Cross-entropy loss\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // Define the architecture\n",
        "    const int input_size = 2;    // Number of inputs\n",
        "    const int hidden_sizes[MAX_LAYERS] = {2, 2};   // Number of neurons in each hidden layer (2 hidden layers with 2 neurons each)\n",
        "    const int num_hidden_layers = 2; // Number of hidden layers\n",
        "    const int output_size = 1;   // Number of outputs\n",
        "\n",
        "    // Initialize input data, weights, biases, and target output\n",
        "    float host_input[input_size] = {9.0f, 9.0f}; // Example input\n",
        "    float host_target[output_size] = {1.0f}; // Target output for training\n",
        "\n",
        "    // Initialize weights and biases for each hidden layer\n",
        "    float host_weights[MAX_LAYERS][input_size * hidden_sizes[0]] = { {0.15f, 0.25f, 0.20f, 0.30f}, {0.40f, 0.45f, 0.50f, 0.55f} }; // Example\n",
        "    float host_biases[MAX_LAYERS][hidden_sizes[0]] = { {0.35f, 0.35f}, {0.60f, 0.60f} }; // Example\n",
        "    float host_output_weights[hidden_sizes[num_hidden_layers-1] * output_size] = {0.40f, 0.50f}; // Example\n",
        "    float host_output_biases[output_size] = {0.60f}; // Example\n",
        "\n",
        "    // Training parameters\n",
        "    const int epochs = 100;\n",
        "    const float learning_rate = 0.01f;\n",
        "\n",
        "    // Allocate memory on the device\n",
        "    float *d_input, *d_weights[MAX_LAYERS], *d_biases[MAX_LAYERS], *d_hidden_output[MAX_LAYERS], *d_output_weights, *d_output_biases, *d_output_output;\n",
        "    float *d_output_delta, *d_hidden_delta[MAX_LAYERS], *d_target, *d_loss;\n",
        "\n",
        "    cudaMalloc((void**)&d_input, input_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_target, output_size * sizeof(float));\n",
        "\n",
        "    // Allocate memory for weights and biases of hidden layers\n",
        "    for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "        cudaMalloc((void**)&d_weights[l], (l == 0 ? input_size : hidden_sizes[l-1]) * hidden_sizes[l] * sizeof(float));\n",
        "        cudaMalloc((void**)&d_biases[l], hidden_sizes[l] * sizeof(float));\n",
        "        cudaMalloc((void**)&d_hidden_output[l], hidden_sizes[l] * sizeof(float));\n",
        "        cudaMalloc((void**)&d_hidden_delta[l], hidden_sizes[l] * sizeof(float));\n",
        "\n",
        "        cudaMemcpy(d_weights[l], host_weights[l], (l == 0 ? input_size : hidden_sizes[l-1]) * hidden_sizes[l] * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(d_biases[l], host_biases[l], hidden_sizes[l] * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    }\n",
        "\n",
        "    cudaMalloc((void**)&d_output_weights, hidden_sizes[num_hidden_layers-1] * output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_biases, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_output, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_output_delta, output_size * sizeof(float));\n",
        "    cudaMalloc((void**)&d_loss, output_size * sizeof(float));\n",
        "\n",
        "    cudaMemcpy(d_output_weights, host_output_weights, hidden_sizes[num_hidden_layers-1] * output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_output_biases, host_output_biases, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Copy input data and target output to device\n",
        "    cudaMemcpy(d_input, host_input, input_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_target, host_target, output_size * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Training loop\n",
        "    for (int epoch = 1; epoch < epochs+1; ++epoch) {\n",
        "        // Forward pass\n",
        "        float *d_input_current = d_input;\n",
        "        for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "            linear_layer_and_activation<<<1, hidden_sizes[l]>>>(d_input_current, d_weights[l], d_biases[l], d_hidden_output[l], (l == 0 ? input_size : hidden_sizes[l-1]), hidden_sizes[l]);\n",
        "            cudaDeviceSynchronize();\n",
        "            d_input_current = d_hidden_output[l];\n",
        "        }\n",
        "\n",
        "        linear_layer_and_activation<<<1, output_size>>>(d_input_current, d_output_weights, d_output_biases, d_output_output, hidden_sizes[num_hidden_layers-1], output_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Compute loss using current weights and biases\n",
        "        compute_loss<<<1, output_size>>>(d_output_output, d_target, d_loss, output_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        float host_loss[output_size];\n",
        "        cudaMemcpy(host_loss, d_loss, output_size * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        float total_loss = 0.0f;\n",
        "        for (int i = 0; i < output_size; ++i) {\n",
        "            total_loss += host_loss[i];\n",
        "        }\n",
        "\n",
        "        // Compute deltas\n",
        "        compute_output_delta<<<1, output_size>>>(d_output_output, d_target, d_output_delta, output_size);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        float *d_delta_current = d_output_delta;\n",
        "        for (int l = num_hidden_layers-1; l >= 0; --l) {\n",
        "            compute_hidden_delta<<<1, hidden_sizes[l]>>>(d_delta_current, (l == num_hidden_layers-1) ? d_output_weights : d_weights[l+1], d_hidden_output[l], d_hidden_delta[l], (l == num_hidden_layers-1) ? output_size : hidden_sizes[l+1], hidden_sizes[l]);\n",
        "            cudaDeviceSynchronize();\n",
        "            d_delta_current = d_hidden_delta[l];\n",
        "        }\n",
        "\n",
        "        // Update weights and biases\n",
        "        float *d_input_previous = d_input;\n",
        "        for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "            update_weights_and_biases<<<1, hidden_sizes[l]>>>(d_weights[l], d_biases[l], d_hidden_delta[l], (l == 0 ? d_input : d_hidden_output[l-1]), (l == 0 ? input_size : hidden_sizes[l-1]), hidden_sizes[l], learning_rate);\n",
        "            cudaDeviceSynchronize();\n",
        "            d_input_previous = d_hidden_output[l];\n",
        "        }\n",
        "\n",
        "        update_weights_and_biases<<<1, output_size>>>(d_output_weights, d_output_biases, d_output_delta, d_input_previous, hidden_sizes[num_hidden_layers-1], output_size, learning_rate);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        if (epoch > 0) {\n",
        "        std::cout << \"Epoch \" << epoch << \" Loss: \" << total_loss << std::endl;\n",
        "    }\n",
        "    }\n",
        "\n",
        "    // Free device memory\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_target);\n",
        "    cudaFree(d_output_weights);\n",
        "    cudaFree(d_output_biases);\n",
        "    cudaFree(d_output_output);\n",
        "    cudaFree(d_output_delta);\n",
        "    cudaFree(d_loss);\n",
        "    for (int l = 0; l < num_hidden_layers; ++l) {\n",
        "        cudaFree(d_weights[l]);\n",
        "        cudaFree(d_biases[l]);\n",
        "        cudaFree(d_hidden_output[l]);\n",
        "        cudaFree(d_hidden_delta[l]);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192fd061-2742-4a05-fba8-2856c4df675f",
        "id": "fSEu_BLTXup4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting back_prop_2.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc back_prop_2.cu -o back_prop_2\n",
        "!./back_prop_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvZBtr_9WQOO",
        "outputId": "8f34b690-ccf9-4bdf-fd7b-dd83e6856d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 0.100841\n",
            "Epoch 2 Loss: 0.100399\n",
            "Epoch 3 Loss: 0.0999591\n",
            "Epoch 4 Loss: 0.0995231\n",
            "Epoch 5 Loss: 0.0990903\n",
            "Epoch 6 Loss: 0.0986608\n",
            "Epoch 7 Loss: 0.0982345\n",
            "Epoch 8 Loss: 0.0978115\n",
            "Epoch 9 Loss: 0.0973917\n",
            "Epoch 10 Loss: 0.096975\n",
            "Epoch 11 Loss: 0.0965613\n",
            "Epoch 12 Loss: 0.0961508\n",
            "Epoch 13 Loss: 0.0957433\n",
            "Epoch 14 Loss: 0.0953388\n",
            "Epoch 15 Loss: 0.0949373\n",
            "Epoch 16 Loss: 0.0945388\n",
            "Epoch 17 Loss: 0.0941432\n",
            "Epoch 18 Loss: 0.0937505\n",
            "Epoch 19 Loss: 0.0933607\n",
            "Epoch 20 Loss: 0.0929737\n",
            "Epoch 21 Loss: 0.0925895\n",
            "Epoch 22 Loss: 0.0922081\n",
            "Epoch 23 Loss: 0.0918295\n",
            "Epoch 24 Loss: 0.0914536\n",
            "Epoch 25 Loss: 0.0910803\n",
            "Epoch 26 Loss: 0.0907098\n",
            "Epoch 27 Loss: 0.0903419\n",
            "Epoch 28 Loss: 0.0899767\n",
            "Epoch 29 Loss: 0.089614\n",
            "Epoch 30 Loss: 0.0892539\n",
            "Epoch 31 Loss: 0.0888963\n",
            "Epoch 32 Loss: 0.0885413\n",
            "Epoch 33 Loss: 0.0881888\n",
            "Epoch 34 Loss: 0.0878387\n",
            "Epoch 35 Loss: 0.0874911\n",
            "Epoch 36 Loss: 0.0871459\n",
            "Epoch 37 Loss: 0.0868031\n",
            "Epoch 38 Loss: 0.0864627\n",
            "Epoch 39 Loss: 0.0861247\n",
            "Epoch 40 Loss: 0.085789\n",
            "Epoch 41 Loss: 0.0854555\n",
            "Epoch 42 Loss: 0.0851244\n",
            "Epoch 43 Loss: 0.0847956\n",
            "Epoch 44 Loss: 0.084469\n",
            "Epoch 45 Loss: 0.0841446\n",
            "Epoch 46 Loss: 0.0838224\n",
            "Epoch 47 Loss: 0.0835024\n",
            "Epoch 48 Loss: 0.0831847\n",
            "Epoch 49 Loss: 0.082869\n",
            "Epoch 50 Loss: 0.0825554\n",
            "Epoch 51 Loss: 0.0822439\n",
            "Epoch 52 Loss: 0.0819346\n",
            "Epoch 53 Loss: 0.0816273\n",
            "Epoch 54 Loss: 0.0813221\n",
            "Epoch 55 Loss: 0.0810188\n",
            "Epoch 56 Loss: 0.0807176\n",
            "Epoch 57 Loss: 0.0804184\n",
            "Epoch 58 Loss: 0.0801211\n",
            "Epoch 59 Loss: 0.0798258\n",
            "Epoch 60 Loss: 0.0795324\n",
            "Epoch 61 Loss: 0.079241\n",
            "Epoch 62 Loss: 0.0789514\n",
            "Epoch 63 Loss: 0.0786638\n",
            "Epoch 64 Loss: 0.078378\n",
            "Epoch 65 Loss: 0.078094\n",
            "Epoch 66 Loss: 0.0778119\n",
            "Epoch 67 Loss: 0.0775316\n",
            "Epoch 68 Loss: 0.0772531\n",
            "Epoch 69 Loss: 0.0769764\n",
            "Epoch 70 Loss: 0.0767015\n",
            "Epoch 71 Loss: 0.0764283\n",
            "Epoch 72 Loss: 0.0761568\n",
            "Epoch 73 Loss: 0.0758871\n",
            "Epoch 74 Loss: 0.0756191\n",
            "Epoch 75 Loss: 0.0753528\n",
            "Epoch 76 Loss: 0.0750881\n",
            "Epoch 77 Loss: 0.0748251\n",
            "Epoch 78 Loss: 0.0745638\n",
            "Epoch 79 Loss: 0.0743041\n",
            "Epoch 80 Loss: 0.074046\n",
            "Epoch 81 Loss: 0.0737895\n",
            "Epoch 82 Loss: 0.0735346\n",
            "Epoch 83 Loss: 0.0732812\n",
            "Epoch 84 Loss: 0.0730295\n",
            "Epoch 85 Loss: 0.0727793\n",
            "Epoch 86 Loss: 0.0725306\n",
            "Epoch 87 Loss: 0.0722835\n",
            "Epoch 88 Loss: 0.0720378\n",
            "Epoch 89 Loss: 0.0717937\n",
            "Epoch 90 Loss: 0.071551\n",
            "Epoch 91 Loss: 0.0713099\n",
            "Epoch 92 Loss: 0.0710702\n",
            "Epoch 93 Loss: 0.0708319\n",
            "Epoch 94 Loss: 0.070595\n",
            "Epoch 95 Loss: 0.0703596\n",
            "Epoch 96 Loss: 0.0701256\n",
            "Epoch 97 Loss: 0.069893\n",
            "Epoch 98 Loss: 0.0696618\n",
            "Epoch 99 Loss: 0.069432\n",
            "Epoch 100 Loss: 0.0692035\n"
          ]
        }
      ]
    }
  ]
}